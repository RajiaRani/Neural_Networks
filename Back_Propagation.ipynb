{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3160caac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 , Loss: 36.0\n",
      "Iteration 2 , Loss: 33.872399999999985\n",
      "Iteration 3 , Loss: 31.870541159999995\n",
      "Iteration 4 , Loss: 29.98699217744401\n",
      "Iteration 5 , Loss: 28.21476093975706\n",
      "Iteration 6 , Loss: 26.54726856821742\n",
      "Iteration 7 , Loss: 24.978324995835766\n",
      "Iteration 8 , Loss: 23.502105988581878\n",
      "Iteration 9 , Loss: 22.113131524656684\n",
      "Iteration 10 , Loss: 20.80624545154949\n",
      "Iteration 11 , Loss: 19.576596345362915\n",
      "Iteration 12 , Loss: 18.419619501351963\n",
      "Iteration 13 , Loss: 17.331019988822064\n",
      "Iteration 14 , Loss: 16.306756707482677\n",
      "Iteration 15 , Loss: 15.343027386070442\n",
      "Iteration 16 , Loss: 14.43625446755368\n",
      "Iteration 17 , Loss: 13.583071828521268\n",
      "Iteration 18 , Loss: 12.780312283455652\n",
      "Iteration 19 , Loss: 12.024995827503426\n",
      "Iteration 20 , Loss: 11.314318574097976\n",
      "Iteration 21 , Loss: 10.645642346368787\n",
      "Iteration 22 , Loss: 10.016484883698395\n",
      "Iteration 23 , Loss: 9.424510627071816\n",
      "Iteration 24 , Loss: 8.867522049011871\n",
      "Iteration 25 , Loss: 8.34345149591527\n",
      "Iteration 26 , Loss: 7.850353512506679\n",
      "Iteration 27 , Loss: 7.386397619917536\n",
      "Iteration 28 , Loss: 6.949861520580408\n",
      "Iteration 29 , Loss: 6.539124704714106\n",
      "Iteration 30 , Loss: 6.152662434665503\n",
      "Iteration 31 , Loss: 5.7890400847767705\n",
      "Iteration 32 , Loss: 5.446907815766464\n",
      "Iteration 33 , Loss: 5.124995563854671\n",
      "Iteration 34 , Loss: 4.8221083260308575\n",
      "Iteration 35 , Loss: 4.537121723962434\n",
      "Iteration 36 , Loss: 4.268977830076255\n",
      "Iteration 37 , Loss: 4.016681240318748\n",
      "Iteration 38 , Loss: 3.7792953790159096\n",
      "Iteration 39 , Loss: 3.5559390221160707\n",
      "Iteration 40 , Loss: 3.345783025909011\n",
      "Iteration 41 , Loss: 3.1480472490777887\n",
      "Iteration 42 , Loss: 2.9619976566572896\n",
      "Iteration 43 , Loss: 2.786943595148845\n",
      "Iteration 44 , Loss: 2.622235228675549\n",
      "Iteration 45 , Loss: 2.4672611266608238\n",
      "Iteration 46 , Loss: 2.321445994075166\n",
      "Iteration 47 , Loss: 2.1842485358253256\n",
      "Iteration 48 , Loss: 2.0551594473580463\n",
      "Iteration 49 , Loss: 1.9336995240191863\n",
      "Iteration 50 , Loss: 1.8194178821496518\n",
      "Iteration 51 , Loss: 1.7118902853146067\n",
      "Iteration 52 , Loss: 1.6107175694525138\n",
      "Iteration 53 , Loss: 1.515524161097869\n",
      "Iteration 54 , Loss: 1.4259566831769857\n",
      "Iteration 55 , Loss: 1.3416826432012259\n",
      "Iteration 56 , Loss: 1.2623891989880334\n",
      "Iteration 57 , Loss: 1.18778199732784\n",
      "Iteration 58 , Loss: 1.1175840812857634\n",
      "Iteration 59 , Loss: 1.0515348620817766\n",
      "Iteration 60 , Loss: 0.9893891517327431\n",
      "Iteration 61 , Loss: 0.930916252865338\n",
      "Iteration 62 , Loss: 0.8758991023209968\n",
      "Iteration 63 , Loss: 0.8241334653738256\n",
      "Iteration 64 , Loss: 0.7754271775702323\n",
      "Iteration 65 , Loss: 0.7295994313758316\n",
      "Iteration 66 , Loss: 0.6864801049815187\n",
      "Iteration 67 , Loss: 0.6459091307771115\n",
      "Iteration 68 , Loss: 0.6077359011481847\n",
      "Iteration 69 , Loss: 0.571818709390327\n",
      "Iteration 70 , Loss: 0.5380242236653578\n",
      "Iteration 71 , Loss: 0.5062269920467349\n",
      "Iteration 72 , Loss: 0.47630897681677353\n",
      "Iteration 73 , Loss: 0.4481591162869011\n",
      "Iteration 74 , Loss: 0.4216729125143454\n",
      "Iteration 75 , Loss: 0.3967520433847474\n",
      "Iteration 76 , Loss: 0.3733039976207088\n",
      "Iteration 77 , Loss: 0.35124173136132436\n",
      "Iteration 78 , Loss: 0.3304833450378702\n",
      "Iteration 79 , Loss: 0.3109517793461322\n",
      "Iteration 80 , Loss: 0.29257452918677535\n",
      "Iteration 81 , Loss: 0.27528337451183676\n",
      "Iteration 82 , Loss: 0.25901412707818716\n",
      "Iteration 83 , Loss: 0.24370639216786655\n",
      "Iteration 84 , Loss: 0.22930334439074554\n",
      "Iteration 85 , Loss: 0.21575151673725296\n",
      "Iteration 86 , Loss: 0.2030006020980815\n",
      "Iteration 87 , Loss: 0.1910032665140846\n",
      "Iteration 88 , Loss: 0.17971497346310225\n",
      "Iteration 89 , Loss: 0.16909381853143338\n",
      "Iteration 90 , Loss: 0.1591003738562249\n",
      "Iteration 91 , Loss: 0.14969754176132236\n",
      "Iteration 92 , Loss: 0.14085041704322837\n",
      "Iteration 93 , Loss: 0.13252615739597357\n",
      "Iteration 94 , Loss: 0.12469386149387143\n",
      "Iteration 95 , Loss: 0.11732445427958357\n",
      "Iteration 96 , Loss: 0.1103905790316602\n",
      "Iteration 97 , Loss: 0.1038664958108892\n",
      "Iteration 98 , Loss: 0.09772798590846558\n",
      "Iteration 99 , Loss: 0.09195226194127534\n",
      "Iteration 100 , Loss: 0.08651788326054576\n",
      "Iteration 101 , Loss: 0.08140467635984766\n",
      "Iteration 102 , Loss: 0.07659365998698062\n",
      "Iteration 103 , Loss: 0.07206697468175022\n",
      "Iteration 104 , Loss: 0.06780781647805834\n",
      "Iteration 105 , Loss: 0.06380037452420508\n",
      "Iteration 106 , Loss: 0.06002977238982451\n",
      "Iteration 107 , Loss: 0.056482012841585764\n",
      "Iteration 108 , Loss: 0.05314392588264784\n",
      "Iteration 109 , Loss: 0.050003119862983315\n",
      "Iteration 110 , Loss: 0.04704793547908108\n",
      "Iteration 111 , Loss: 0.044267402492267266\n",
      "Iteration 112 , Loss: 0.04165119900497404\n",
      "Iteration 113 , Loss: 0.03918961314378035\n",
      "Iteration 114 , Loss: 0.036873507006982977\n",
      "Iteration 115 , Loss: 0.034694282742870286\n",
      "Iteration 116 , Loss: 0.032643850632766785\n",
      "Iteration 117 , Loss: 0.030714599060370322\n",
      "Iteration 118 , Loss: 0.028899366255902493\n",
      "Iteration 119 , Loss: 0.027191413710178584\n",
      "Iteration 120 , Loss: 0.025584401159906914\n",
      "Iteration 121 , Loss: 0.024072363051356495\n",
      "Iteration 122 , Loss: 0.02264968639502138\n",
      "Iteration 123 , Loss: 0.021311089929075627\n",
      "Iteration 124 , Loss: 0.02005160451426725\n",
      "Iteration 125 , Loss: 0.018866554687474075\n",
      "Iteration 126 , Loss: 0.017751541305444478\n",
      "Iteration 127 , Loss: 0.016702425214292563\n",
      "Iteration 128 , Loss: 0.015715311884128023\n",
      "Iteration 129 , Loss: 0.014786536951776086\n",
      "Iteration 130 , Loss: 0.013912652617925996\n",
      "Iteration 131 , Loss: 0.013090414848206543\n",
      "Iteration 132 , Loss: 0.012316771330677616\n",
      "Iteration 133 , Loss: 0.011588850145034585\n",
      "Iteration 134 , Loss: 0.010903949101463065\n",
      "Iteration 135 , Loss: 0.010259525709566468\n",
      "Iteration 136 , Loss: 0.00965318774013127\n",
      "Iteration 137 , Loss: 0.009082684344689475\n",
      "Iteration 138 , Loss: 0.008545897699918217\n",
      "Iteration 139 , Loss: 0.008040835145853157\n",
      "Iteration 140 , Loss: 0.007565621788733219\n",
      "Iteration 141 , Loss: 0.0071184935410191314\n",
      "Iteration 142 , Loss: 0.0066977905727448606\n",
      "Iteration 143 , Loss: 0.0063019511498957235\n",
      "Iteration 144 , Loss: 0.0059295058369368625\n",
      "Iteration 145 , Loss: 0.005579072041973911\n",
      "Iteration 146 , Loss: 0.005249348884293189\n",
      "Iteration 147 , Loss: 0.004939112365231465\n",
      "Iteration 148 , Loss: 0.004647210824446307\n",
      "Iteration 149 , Loss: 0.004372560664721486\n",
      "Iteration 150 , Loss: 0.004114142329436494\n",
      "Iteration 151 , Loss: 0.003870996517766834\n",
      "Iteration 152 , Loss: 0.0036422206235667827\n",
      "Iteration 153 , Loss: 0.003426965384714017\n",
      "Iteration 154 , Loss: 0.0032244317304774505\n",
      "Iteration 155 , Loss: 0.0030338678152062068\n",
      "Iteration 156 , Loss: 0.0028545662273275238\n",
      "Iteration 157 , Loss: 0.002685861363292443\n",
      "Iteration 158 , Loss: 0.002527126956721865\n",
      "Iteration 159 , Loss: 0.0023777737535795864\n",
      "Iteration 160 , Loss: 0.00223724732474303\n",
      "Iteration 161 , Loss: 0.0021050260078507234\n",
      "Iteration 162 , Loss: 0.0019806189707867374\n",
      "Iteration 163 , Loss: 0.0018635643896132343\n",
      "Iteration 164 , Loss: 0.0017534277341871227\n",
      "Iteration 165 , Loss: 0.001649800155096641\n",
      "Iteration 166 , Loss: 0.0015522969659304577\n",
      "Iteration 167 , Loss: 0.001460556215243966\n",
      "Iteration 168 , Loss: 0.0013742373429230384\n",
      "Iteration 169 , Loss: 0.0012930199159562866\n",
      "Iteration 170 , Loss: 0.0012166024389232565\n",
      "Iteration 171 , Loss: 0.0011447012347829176\n",
      "Iteration 172 , Loss: 0.0010770493918072417\n",
      "Iteration 173 , Loss: 0.0010133957727514104\n",
      "Iteration 174 , Loss: 0.0009535040825818078\n",
      "Iteration 175 , Loss: 0.0008971519913012032\n",
      "Iteration 176 , Loss: 0.0008441303086153036\n",
      "Iteration 177 , Loss: 0.0007942422073761319\n",
      "Iteration 178 , Loss: 0.0007473024929201971\n",
      "Iteration 179 , Loss: 0.0007031369155886336\n",
      "Iteration 180 , Loss: 0.0006615815238773228\n",
      "Iteration 181 , Loss: 0.0006224820558161947\n",
      "Iteration 182 , Loss: 0.0005856933663174669\n",
      "Iteration 183 , Loss: 0.0005510788883681015\n",
      "Iteration 184 , Loss: 0.0005185101260655451\n",
      "Iteration 185 , Loss: 0.00048786617761505856\n",
      "Iteration 186 , Loss: 0.00045903328651801555\n",
      "Iteration 187 , Loss: 0.0004319044192847635\n",
      "Iteration 188 , Loss: 0.0004063788681050637\n",
      "Iteration 189 , Loss: 0.0003823618770000461\n",
      "Iteration 190 , Loss: 0.0003597642900693548\n",
      "Iteration 191 , Loss: 0.0003385022205262612\n",
      "Iteration 192 , Loss: 0.00031849673929316324\n",
      "Iteration 193 , Loss: 0.0002996735820009465\n",
      "Iteration 194 , Loss: 0.0002819628733046985\n",
      "Iteration 195 , Loss: 0.0002652988674923804\n",
      "Iteration 196 , Loss: 0.0002496197044235683\n",
      "Iteration 197 , Loss: 0.00023486717989212869\n",
      "Iteration 198 , Loss: 0.00022098652956051694\n",
      "Iteration 199 , Loss: 0.0002079262256634926\n",
      "Iteration 200 , Loss: 0.00019563778572677352\n",
      "Final weights:  [-3.3990955  -0.20180899  0.80271349]\n",
      "Final bias:  {np.float64(0.6009044964039992)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Intitial Parameters\n",
    "weights = np.array([-3.0, -1.0, 2.0])\n",
    "bias = 1.0\n",
    "inputs = np.array([1.0, -2.0, 3.0])\n",
    "target_range = 0.0\n",
    "learning_rate = 0.001 #steps\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x>0, 1.0, 0.0)\n",
    "\n",
    "for iteration in range(200):\n",
    "\n",
    "    #Forward pass\n",
    "    sum_output = np.dot(weights, inputs) + bias\n",
    "    relu_output = relu(sum_output)\n",
    "    loss = (relu_output - target_range) ** 2\n",
    "\n",
    "    #Backward pass\n",
    "    # defficients\n",
    "    dloss_doutput = 2 * (relu_output - target_range)\n",
    "    drelu_output = relu_derivative(sum_output)\n",
    "    dweights_output = inputs\n",
    "    dbias_output = 1.0\n",
    "\n",
    "\n",
    "    dfinal_dloss = dloss_doutput * drelu_output \n",
    "    dloss_dweights = dfinal_dloss * dweights_output\n",
    "    dloss_dbias =  dfinal_dloss * dbias_output\n",
    "\n",
    "\n",
    "    #Update weights and biases\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    bias -= learning_rate * dloss_dbias\n",
    "\n",
    "    #Print the loss for this iteration\n",
    "    print(f\"Iteration {iteration + 1} , Loss: {loss}\")\n",
    "\n",
    "print(\"Final weights: \", weights)\n",
    "print(\"Final bias: \" , {bias})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b26d6",
   "metadata": {},
   "source": [
    "BACKPROPAGATION WITH MULTIPLE NEURONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe7e5643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 , Loss: 466.56000000000006\n",
      "Iteration 1 , Loss: 309.14078976\n",
      "Iteration 2 , Loss: 204.83545073181696\n",
      "Iteration 3 , Loss: 140.8182193826751\n",
      "Iteration 4 , Loss: 108.06052191699968\n",
      "Iteration 5 , Loss: 82.92305106657953\n",
      "Iteration 6 , Loss: 63.63315923526753\n",
      "Iteration 7 , Loss: 48.83055920132266\n",
      "Iteration 8 , Loss: 37.47139919767418\n",
      "Iteration 9 , Loss: 28.754652430714412\n",
      "Iteration 10 , Loss: 22.06563016367191\n",
      "Iteration 11 , Loss: 16.932635012477895\n",
      "Iteration 12 , Loss: 14.840512693931485\n",
      "Iteration 13 , Loss: 13.057336050679451\n",
      "Iteration 14 , Loss: 11.488418780174012\n",
      "Iteration 15 , Loss: 10.108016333223427\n",
      "Iteration 16 , Loss: 8.89347752268863\n",
      "Iteration 17 , Loss: 7.824872837472455\n",
      "Iteration 18 , Loss: 6.884667416813114\n",
      "Iteration 19 , Loss: 6.057433318678514\n",
      "Iteration 20 , Loss: 5.32959636083938\n",
      "Iteration 21 , Loss: 4.689213380506364\n",
      "Iteration 22 , Loss: 4.1257762575582415\n",
      "Iteration 23 , Loss: 3.630039485555074\n",
      "Iteration 24 , Loss: 3.1938684611287185\n",
      "Iteration 25 , Loss: 2.810106002313336\n",
      "Iteration 26 , Loss: 2.472454905499375\n",
      "Iteration 27 , Loss: 2.1753746138741916\n",
      "Iteration 28 , Loss: 1.9139903017695257\n",
      "Iteration 29 , Loss: 1.6840128830701064\n",
      "Iteration 30 , Loss: 1.481668631091935\n",
      "Iteration 31 , Loss: 1.3036372550544522\n",
      "Iteration 32 , Loss: 1.1469974170361295\n",
      "Iteration 33 , Loss: 1.0091787953947366\n",
      "Iteration 34 , Loss: 0.8879199080552862\n",
      "Iteration 35 , Loss: 0.7812310035829951\n",
      "Iteration 36 , Loss: 0.687361411116477\n",
      "Iteration 37 , Loss: 0.6047708134023655\n",
      "Iteration 38 , Loss: 0.5321039715471908\n",
      "Iteration 39 , Loss: 0.4681684867419666\n",
      "Iteration 40 , Loss: 0.41191523404899866\n",
      "Iteration 41 , Loss: 0.3624211471866072\n",
      "Iteration 42 , Loss: 0.3188740718252532\n",
      "Iteration 43 , Loss: 0.2805594388510181\n",
      "Iteration 44 , Loss: 0.2468485389164351\n",
      "Iteration 45 , Loss: 0.21718820587439186\n",
      "Iteration 46 , Loss: 0.1910917398093484\n",
      "Iteration 47 , Loss: 0.16813092072081634\n",
      "Iteration 48 , Loss: 0.14792898181068598\n",
      "Iteration 49 , Loss: 0.13015442707224115\n",
      "Iteration 50 , Loss: 0.11451559173294901\n",
      "Iteration 51 , Loss: 0.10075585629268477\n",
      "Iteration 52 , Loss: 0.0886494356239809\n",
      "Iteration 53 , Loss: 0.07799767403714583\n",
      "Iteration 54 , Loss: 0.06862578551553851\n",
      "Iteration 55 , Loss: 0.060379985631133434\n",
      "Iteration 56 , Loss: 0.05312496807763904\n",
      "Iteration 57 , Loss: 0.0467416844133022\n",
      "Iteration 58 , Loss: 0.041125390580937476\n",
      "Iteration 59 , Loss: 0.03618392815029437\n",
      "Iteration 60 , Loss: 0.031836212079467595\n",
      "Iteration 61 , Loss: 0.028010900180847065\n",
      "Iteration 62 , Loss: 0.024645222458717243\n",
      "Iteration 63 , Loss: 0.021683951108967602\n",
      "Iteration 64 , Loss: 0.019078494279518517\n",
      "Iteration 65 , Loss: 0.016786098720868715\n",
      "Iteration 66 , Loss: 0.01476914824296401\n",
      "Iteration 67 , Loss: 0.012994546466682442\n",
      "Iteration 68 , Loss: 0.01143317374143173\n",
      "Iteration 69 , Loss: 0.010059409317356252\n",
      "Iteration 70 , Loss: 0.008850710931420008\n",
      "Iteration 71 , Loss: 0.007787244908744308\n",
      "Iteration 72 , Loss: 0.006851560709489224\n",
      "Iteration 73 , Loss: 0.006028304580879819\n",
      "Iteration 74 , Loss: 0.005303967615659622\n",
      "Iteration 75 , Loss: 0.004666664082832435\n",
      "Iteration 76 , Loss: 0.00410593639329562\n",
      "Iteration 77 , Loss: 0.0036125835000228004\n",
      "Iteration 78 , Loss: 0.003178509916994065\n",
      "Iteration 79 , Loss: 0.0027965928794077147\n",
      "Iteration 80 , Loss: 0.002460565465389601\n",
      "Iteration 81 , Loss: 0.0021649137613302567\n",
      "Iteration 82 , Loss: 0.0019047863834238405\n",
      "Iteration 83 , Loss: 0.0016759148707371741\n",
      "Iteration 84 , Loss: 0.001474543643528874\n",
      "Iteration 85 , Loss: 0.0012973683774970255\n",
      "Iteration 86 , Loss: 0.0011414817827304954\n",
      "Iteration 87 , Loss: 0.0010043258976447365\n",
      "Iteration 88 , Loss: 0.0008836501150873324\n",
      "Iteration 89 , Loss: 0.0007774742518588998\n",
      "Iteration 90 , Loss: 0.0006840560556525385\n",
      "Iteration 91 , Loss: 0.0006018626162295437\n",
      "Iteration 92 , Loss: 0.0005295452117138669\n",
      "Iteration 93 , Loss: 0.00046591717725518237\n",
      "Iteration 94 , Loss: 0.0004099344329049093\n",
      "Iteration 95 , Loss: 0.00036067835118478575\n",
      "Iteration 96 , Loss: 0.00031734068321982587\n",
      "Iteration 97 , Loss: 0.0002792102960868692\n",
      "Iteration 98 , Loss: 0.00024566150375025336\n",
      "Iteration 99 , Loss: 0.0002161438001056386\n",
      "Iteration 100 , Loss: 0.000190172825660145\n",
      "Iteration 101 , Loss: 0.00016732241962012735\n",
      "Iteration 102 , Loss: 0.00014721762696825352\n",
      "Iteration 103 , Loss: 0.00012952854578225566\n",
      "Iteration 104 , Loss: 0.00011396491383524246\n",
      "Iteration 105 , Loss: 0.00010027134564845534\n",
      "Iteration 106 , Loss: 8.822314184071818e-05\n",
      "Iteration 107 , Loss: 7.76226020097066e-05\n",
      "Iteration 108 , Loss: 6.82957806426276e-05\n",
      "Iteration 109 , Loss: 6.008963282373209e-05\n",
      "Iteration 110 , Loss: 5.28695029021632e-05\n",
      "Iteration 111 , Loss: 4.6516914911452744e-05\n",
      "Iteration 112 , Loss: 4.0927628483352786e-05\n",
      "Iteration 113 , Loss: 3.6009928355305675e-05\n",
      "Iteration 114 , Loss: 3.1683119403846986e-05\n",
      "Iteration 115 , Loss: 2.787620250875636e-05\n",
      "Iteration 116 , Loss: 2.4526709520114518e-05\n",
      "Iteration 117 , Loss: 2.157967821101559e-05\n",
      "Iteration 118 , Loss: 1.898675039589321e-05\n",
      "Iteration 119 , Loss: 1.6705378415325086e-05\n",
      "Iteration 120 , Loss: 1.4698126966451542e-05\n",
      "Iteration 121 , Loss: 1.293205882267119e-05\n",
      "Iteration 122 , Loss: 1.1378194362774346e-05\n",
      "Iteration 123 , Loss: 1.0011036040919489e-05\n",
      "Iteration 124 , Loss: 8.80814999438679e-06\n",
      "Iteration 125 , Loss: 7.749797923661431e-06\n",
      "Iteration 126 , Loss: 6.818613204344925e-06\n",
      "Iteration 127 , Loss: 5.999315916163863e-06\n",
      "Iteration 128 , Loss: 5.278462112941122e-06\n",
      "Iteration 129 , Loss: 4.644223219297674e-06\n",
      "Iteration 130 , Loss: 4.086191934160694e-06\n",
      "Iteration 131 , Loss: 3.5952114561194955e-06\n",
      "Iteration 132 , Loss: 3.163225228398182e-06\n",
      "Iteration 133 , Loss: 2.7831447378551217e-06\n",
      "Iteration 134 , Loss: 2.4487331987338138e-06\n",
      "Iteration 135 , Loss: 2.1545032125068455e-06\n",
      "Iteration 136 , Loss: 1.895626724504719e-06\n",
      "Iteration 137 , Loss: 1.6678557997948718e-06\n",
      "Iteration 138 , Loss: 1.4674529183147234e-06\n",
      "Iteration 139 , Loss: 1.2911296454616241e-06\n",
      "Iteration 140 , Loss: 1.1359926717815175e-06\n",
      "Iteration 141 , Loss: 9.994963363109655e-07\n",
      "Iteration 142 , Loss: 8.794008545252129e-07\n",
      "Iteration 143 , Loss: 7.737355654488364e-07\n",
      "Iteration 144 , Loss: 6.807665948467607e-07\n",
      "Iteration 145 , Loss: 5.989684038763183e-07\n",
      "Iteration 146 , Loss: 5.269987563400152e-07\n",
      "Iteration 147 , Loss: 4.636766937733644e-07\n",
      "Iteration 148 , Loss: 4.0796315695611795e-07\n",
      "Iteration 149 , Loss: 3.5894393586886055e-07\n",
      "Iteration 150 , Loss: 3.1581466831063177e-07\n",
      "Iteration 151 , Loss: 2.7786764102513465e-07\n",
      "Iteration 152 , Loss: 2.444801767501443e-07\n",
      "Iteration 153 , Loss: 2.1510441663245662e-07\n",
      "Iteration 154 , Loss: 1.892583303474886e-07\n",
      "Iteration 155 , Loss: 1.6651780640630245e-07\n",
      "Iteration 156 , Loss: 1.4650969285977134e-07\n",
      "Iteration 157 , Loss: 1.289056742045843e-07\n",
      "Iteration 158 , Loss: 1.1341688401476149e-07\n",
      "Iteration 159 , Loss: 9.978916489917351e-08\n",
      "Iteration 160 , Loss: 8.779889800154524e-08\n",
      "Iteration 161 , Loss: 7.72493336133102e-08\n",
      "Iteration 162 , Loss: 6.796736268358103e-08\n",
      "Iteration 163 , Loss: 5.980067625299646e-08\n",
      "Iteration 164 , Loss: 5.261526619713125e-08\n",
      "Iteration 165 , Loss: 4.629322627196534e-08\n",
      "Iteration 166 , Loss: 4.073081737599484e-08\n",
      "Iteration 167 , Loss: 3.5836765283401785e-08\n",
      "Iteration 168 , Loss: 3.153076291402601e-08\n",
      "Iteration 169 , Loss: 2.77421525653483e-08\n",
      "Iteration 170 , Loss: 2.4408766481736144e-08\n",
      "Iteration 171 , Loss: 2.14759067363383e-08\n",
      "Iteration 172 , Loss: 1.8895447686540504e-08\n",
      "Iteration 173 , Loss: 1.6625046274292638e-08\n",
      "Iteration 174 , Loss: 1.4627447214131197e-08\n",
      "Iteration 175 , Loss: 1.2869871666684987e-08\n",
      "Iteration 176 , Loss: 1.1323479366681083e-08\n",
      "Iteration 177 , Loss: 9.962895379898372e-09\n",
      "Iteration 178 , Loss: 8.765793722617453e-09\n",
      "Iteration 179 , Loss: 7.712531012094228e-09\n",
      "Iteration 180 , Loss: 6.7858241357822796e-09\n",
      "Iteration 181 , Loss: 5.970466650935363e-09\n",
      "Iteration 182 , Loss: 5.253079260026447e-09\n",
      "Iteration 183 , Loss: 4.621890268470638e-09\n",
      "Iteration 184 , Loss: 4.066542421370032e-09\n",
      "Iteration 185 , Loss: 3.5779229501800417e-09\n",
      "Iteration 186 , Loss: 3.1480140401832544e-09\n",
      "Iteration 187 , Loss: 2.769761265155462e-09\n",
      "Iteration 188 , Loss: 2.4369578305909104e-09\n",
      "Iteration 189 , Loss: 2.1441427254992643e-09\n",
      "Iteration 190 , Loss: 1.8865111121738734e-09\n",
      "Iteration 191 , Loss: 1.6598354829854144e-09\n",
      "Iteration 192 , Loss: 1.4603962906971435e-09\n",
      "Iteration 193 , Loss: 1.284920913992261e-09\n",
      "Iteration 194 , Loss: 1.1305299566400613e-09\n",
      "Iteration 195 , Loss: 9.946899991675513e-10\n",
      "Iteration 196 , Loss: 8.751720276272285e-10\n",
      "Iteration 197 , Loss: 7.700148574790648e-10\n",
      "Iteration 198 , Loss: 6.774929522716155e-10\n",
      "Iteration 199 , Loss: 5.960881091030875e-10\n",
      "Iteration 200 , Loss: 5.244645462659219e-10\n",
      "Iteration 201 , Loss: 4.6144698424295505e-10\n",
      "Iteration 202 , Loss: 4.0600136039994214e-10\n",
      "Iteration 203 , Loss: 3.572178609422143e-10\n",
      "Iteration 204 , Loss: 3.1429599164381096e-10\n",
      "Iteration 205 , Loss: 2.7653144247199196e-10\n",
      "Iteration 206 , Loss: 2.433045304737103e-10\n",
      "Iteration 207 , Loss: 2.1407003131101026e-10\n",
      "Iteration 208 , Loss: 1.8834823262928904e-10\n",
      "Iteration 209 , Loss: 1.657170623934159e-10\n",
      "Iteration 210 , Loss: 1.4580516304364147e-10\n",
      "Iteration 211 , Loss: 1.282857978764051e-10\n",
      "Iteration 212 , Loss: 1.1287148954673651e-10\n",
      "Iteration 213 , Loss: 9.930930284808506e-11\n",
      "Iteration 214 , Loss: 8.73766942571975e-11\n",
      "Iteration 215 , Loss: 7.687786018349664e-11\n",
      "Iteration 216 , Loss: 6.764052401730498e-11\n",
      "Iteration 217 , Loss: 5.951310921205437e-11\n",
      "Iteration 218 , Loss: 5.236225206464649e-11\n",
      "Iteration 219 , Loss: 4.607061330732452e-11\n",
      "Iteration 220 , Loss: 4.0534952695210353e-11\n",
      "Iteration 221 , Loss: 3.5664434919996756e-11\n",
      "Iteration 222 , Loss: 3.137913907908239e-11\n",
      "Iteration 223 , Loss: 2.7608747245003665e-11\n",
      "Iteration 224 , Loss: 2.4291390610848877e-11\n",
      "Iteration 225 , Loss: 2.1372634280224204e-11\n",
      "Iteration 226 , Loss: 1.8804584035578572e-11\n",
      "Iteration 227 , Loss: 1.6545100435475016e-11\n",
      "Iteration 228 , Loss: 1.4557107347254625e-11\n",
      "Iteration 229 , Loss: 1.280798355816041e-11\n",
      "Iteration 230 , Loss: 1.1269027485178472e-11\n",
      "Iteration 231 , Loss: 9.91498621824829e-12\n",
      "Iteration 232 , Loss: 8.723641134507177e-12\n",
      "Iteration 233 , Loss: 7.675443310693672e-12\n",
      "Iteration 234 , Loss: 6.753192744730297e-12\n",
      "Iteration 235 , Loss: 5.941756117191646e-12\n",
      "Iteration 236 , Loss: 5.227818468617682e-12\n",
      "Iteration 237 , Loss: 4.5996647121886195e-12\n",
      "Iteration 238 , Loss: 4.046987399187784e-12\n",
      "Iteration 239 , Loss: 3.5607175808051657e-12\n",
      "Iteration 240 , Loss: 3.132875999354687e-12\n",
      "Iteration 241 , Loss: 2.7564421506948454e-12\n",
      "Iteration 242 , Loss: 2.4252390873398696e-12\n",
      "Iteration 243 , Loss: 2.1338320594457466e-12\n",
      "Iteration 244 , Loss: 1.8774393344897623e-12\n",
      "Iteration 245 , Loss: 1.6518537339027793e-12\n",
      "Iteration 246 , Loss: 1.453373596696527e-12\n",
      "Iteration 247 , Loss: 1.2787420390360837e-12\n",
      "Iteration 248 , Loss: 1.1250935105959603e-12\n",
      "Iteration 249 , Loss: 9.899067747278448e-13\n",
      "Iteration 250 , Loss: 8.709635364409836e-13\n",
      "Iteration 251 , Loss: 7.663120419616907e-13\n",
      "Iteration 252 , Loss: 6.742350523170252e-13\n",
      "Iteration 253 , Loss: 5.932216652276764e-13\n",
      "Iteration 254 , Loss: 5.219425226822998e-13\n",
      "Iteration 255 , Loss: 4.592279970647551e-13\n",
      "Iteration 256 , Loss: 4.040489977362462e-13\n",
      "Iteration 257 , Loss: 3.5550008638655774e-13\n",
      "Iteration 258 , Loss: 3.1278461793066336e-13\n",
      "Iteration 259 , Loss: 2.7520166947290934e-13\n",
      "Iteration 260 , Loss: 2.42134537657066e-13\n",
      "Iteration 261 , Loss: 2.1304062012833435e-13\n",
      "Iteration 262 , Loss: 1.8744251144992854e-13\n",
      "Iteration 263 , Loss: 1.649201689861696e-13\n",
      "Iteration 264 , Loss: 1.451040212383528e-13\n",
      "Iteration 265 , Loss: 1.2766890246963723e-13\n",
      "Iteration 266 , Loss: 1.1232871775573205e-13\n",
      "Iteration 267 , Loss: 9.883174834613884e-14\n",
      "Iteration 268 , Loss: 8.695652082625218e-14\n",
      "Iteration 269 , Loss: 7.650817314102254e-14\n",
      "Iteration 270 , Loss: 6.731525707693416e-14\n",
      "Iteration 271 , Loss: 5.922692499033121e-14\n",
      "Iteration 272 , Loss: 5.2110454583741885e-14\n",
      "Iteration 273 , Loss: 4.5849070772206315e-14\n",
      "Iteration 274 , Loss: 4.03400298259728e-14\n",
      "Iteration 275 , Loss: 3.549293317374683e-14\n",
      "Iteration 276 , Loss: 3.122824429812767e-14\n",
      "Iteration 277 , Loss: 2.747598338158789e-14\n",
      "Iteration 278 , Loss: 2.4174579166942893e-14\n",
      "Iteration 279 , Loss: 2.126985840344702e-14\n",
      "Iteration 280 , Loss: 1.8714157315955333e-14\n",
      "Iteration 281 , Loss: 1.6465539063177358e-14\n",
      "Iteration 282 , Loss: 1.4487105714740854e-14\n",
      "Iteration 283 , Loss: 1.2746393028773312e-14\n",
      "Iteration 284 , Loss: 1.1214837430835652e-14\n",
      "Iteration 285 , Loss: 9.867307425990501e-15\n",
      "Iteration 286 , Loss: 8.68169126890544e-15\n",
      "Iteration 287 , Loss: 7.638533979338799e-15\n",
      "Iteration 288 , Loss: 6.7207182924560076e-15\n",
      "Iteration 289 , Loss: 5.9131836786386695e-15\n",
      "Iteration 290 , Loss: 5.202679202329906e-15\n",
      "Iteration 291 , Loss: 4.577546076631952e-15\n",
      "Iteration 292 , Loss: 4.02752645027987e-15\n",
      "Iteration 293 , Loss: 3.5435949925489602e-15\n",
      "Iteration 294 , Loss: 3.117810777922863e-15\n",
      "Iteration 295 , Loss: 2.743187101767428e-15\n",
      "Iteration 296 , Loss: 2.4135767170827224e-15\n",
      "Iteration 297 , Loss: 2.1235710025246998e-15\n",
      "Iteration 298 , Loss: 1.8684111979301167e-15\n",
      "Iteration 299 , Loss: 1.6439103758759372e-15\n",
      "Iteration 300 , Loss: 1.446384677446104e-15\n",
      "Iteration 301 , Loss: 1.2725928852836776e-15\n",
      "Iteration 302 , Loss: 1.1196832071611266e-15\n",
      "Iteration 303 , Loss: 9.851465479562971e-16\n",
      "Iteration 304 , Loss: 8.667752707011164e-16\n",
      "Iteration 305 , Loss: 7.626270201096887e-16\n",
      "Iteration 306 , Loss: 6.709928140558806e-16\n",
      "Iteration 307 , Loss: 5.903690058818189e-16\n",
      "Iteration 308 , Loss: 5.194326277274777e-16\n",
      "Iteration 309 , Loss: 4.570196788975803e-16\n",
      "Iteration 310 , Loss: 4.0210602417994967e-16\n",
      "Iteration 311 , Loss: 3.5379056757641934e-16\n",
      "Iteration 312 , Loss: 3.1128050919449083e-16\n",
      "Iteration 313 , Loss: 2.7387828641629285e-16\n",
      "Iteration 314 , Loss: 2.409701687639734e-16\n",
      "Iteration 315 , Loss: 2.1201615977915597e-16\n",
      "Iteration 316 , Loss: 1.8654114582088798e-16\n",
      "Iteration 317 , Loss: 1.6412710866386878e-16\n",
      "Iteration 318 , Loss: 1.4440625023363553e-16\n",
      "Iteration 319 , Loss: 1.2705497523643426e-16\n",
      "Iteration 320 , Loss: 1.1178855738131043e-16\n",
      "Iteration 321 , Loss: 9.835649358362996e-17\n",
      "Iteration 322 , Loss: 8.653836952344513e-17\n",
      "Iteration 323 , Loss: 7.614026593851146e-17\n",
      "Iteration 324 , Loss: 6.69915547145651e-17\n",
      "Iteration 325 , Loss: 5.894211820911363e-17\n",
      "Iteration 326 , Loss: 5.1859868796935454e-17\n",
      "Iteration 327 , Loss: 4.5628593959678953e-17\n",
      "Iteration 328 , Loss: 4.014604545568629e-17\n",
      "Iteration 329 , Loss: 3.532225705691344e-17\n",
      "Iteration 330 , Loss: 3.107807579160743e-17\n",
      "Iteration 331 , Loss: 2.7343858073250342e-17\n",
      "Iteration 332 , Loss: 2.4058329357229187e-17\n",
      "Iteration 333 , Loss: 2.1167577133145893e-17\n",
      "Iteration 334 , Loss: 1.862416477544867e-17\n",
      "Iteration 335 , Loss: 1.638635886665511e-17\n",
      "Iteration 336 , Loss: 1.441744058435017e-17\n",
      "Iteration 337 , Loss: 1.268509826569493e-17\n",
      "Iteration 338 , Loss: 1.1160907071058467e-17\n",
      "Iteration 339 , Loss: 9.81985786119992e-18\n",
      "Iteration 340 , Loss: 8.639942892432007e-18\n",
      "Iteration 341 , Loss: 7.601801784920157e-18\n",
      "Iteration 342 , Loss: 6.688399523692915e-18\n",
      "Iteration 343 , Loss: 5.884748121711927e-18\n",
      "Iteration 344 , Loss: 5.177660238359559e-18\n",
      "Iteration 345 , Loss: 4.555532987300234e-18\n",
      "Iteration 346 , Loss: 4.008158128737379e-18\n",
      "Iteration 347 , Loss: 3.526553712473526e-18\n",
      "Iteration 348 , Loss: 3.1028169852583987e-18\n",
      "Iteration 349 , Loss: 2.7299952402439243e-18\n",
      "Iteration 350 , Loss: 2.401969718021561e-18\n",
      "Iteration 351 , Loss: 2.1133586658874336e-18\n",
      "Iteration 352 , Loss: 1.859426032409411e-18\n",
      "Iteration 353 , Loss: 1.6360045932432434e-18\n",
      "Iteration 354 , Loss: 1.439428552343428e-18\n",
      "Iteration 355 , Loss: 1.2664724861244741e-18\n",
      "Iteration 356 , Loss: 1.1142980909245845e-18\n",
      "Iteration 357 , Loss: 9.804086547353087e-19\n",
      "Iteration 358 , Loss: 8.626070719828111e-19\n",
      "Iteration 359 , Loss: 7.589596756956288e-19\n",
      "Iteration 360 , Loss: 6.677661257483395e-19\n",
      "Iteration 361 , Loss: 5.875297635901799e-19\n",
      "Iteration 362 , Loss: 5.169346558134772e-19\n",
      "Iteration 363 , Loss: 4.548219597587547e-19\n",
      "Iteration 364 , Loss: 4.001723658304247e-19\n",
      "Iteration 365 , Loss: 3.5208923205048323e-19\n",
      "Iteration 366 , Loss: 3.0978355663572947e-19\n",
      "Iteration 367 , Loss: 2.725612853887852e-19\n",
      "Iteration 368 , Loss: 2.3981135280956297e-19\n",
      "Iteration 369 , Loss: 2.1099652178985684e-19\n",
      "Iteration 370 , Loss: 1.8564411898217269e-19\n",
      "Iteration 371 , Loss: 1.633378548215379e-19\n",
      "Iteration 372 , Loss: 1.4371191830166731e-19\n",
      "Iteration 373 , Loss: 1.264440125723779e-19\n",
      "Iteration 374 , Loss: 1.1125100609397705e-19\n",
      "Iteration 375 , Loss: 9.788341269789129e-20\n",
      "Iteration 376 , Loss: 8.61222207283828e-20\n",
      "Iteration 377 , Loss: 7.577413896293406e-20\n",
      "Iteration 378 , Loss: 6.66693772034142e-20\n",
      "Iteration 379 , Loss: 5.86586937186109e-20\n",
      "Iteration 380 , Loss: 5.161046100041788e-20\n",
      "Iteration 381 , Loss: 4.5409201031484996e-20\n",
      "Iteration 382 , Loss: 3.9953013582748435e-20\n",
      "Iteration 383 , Loss: 3.5152415244483604e-20\n",
      "Iteration 384 , Loss: 3.092866040187845e-20\n",
      "Iteration 385 , Loss: 2.7212407289609375e-20\n",
      "Iteration 386 , Loss: 2.394259033084349e-20\n",
      "Iteration 387 , Loss: 2.1065715482460667e-20\n",
      "Iteration 388 , Loss: 1.8534557376775318e-20\n",
      "Iteration 389 , Loss: 1.630749997497048e-20\n",
      "Iteration 390 , Loss: 1.4348044265293868e-20\n",
      "Iteration 391 , Loss: 1.2624032787393705e-20\n",
      "Iteration 392 , Loss: 1.110718421332692e-20\n",
      "Iteration 393 , Loss: 9.772600609203139e-21\n",
      "Iteration 394 , Loss: 8.598367248120342e-21\n",
      "Iteration 395 , Loss: 7.565239079626387e-21\n",
      "Iteration 396 , Loss: 6.656228143059702e-21\n",
      "Iteration 397 , Loss: 5.856433770613933e-21\n",
      "Iteration 398 , Loss: 5.1527447513543475e-21\n",
      "Iteration 399 , Loss: 4.533605845571494e-21\n",
      "Iteration 400 , Loss: 3.988856666940334e-21\n",
      "Iteration 401 , Loss: 3.509561969759586e-21\n",
      "Iteration 402 , Loss: 3.0878544685952796e-21\n",
      "Iteration 403 , Loss: 2.7168418875619992e-21\n",
      "Iteration 404 , Loss: 2.390387105794704e-21\n",
      "Iteration 405 , Loss: 2.103173633416929e-21\n",
      "Iteration 406 , Loss: 1.85045927695903e-21\n",
      "Iteration 407 , Loss: 1.6281225902329244e-21\n",
      "Iteration 408 , Loss: 1.4324891524225839e-21\n",
      "Iteration 409 , Loss: 1.2603617495804443e-21\n",
      "Iteration 410 , Loss: 1.1089233831943815e-21\n",
      "Iteration 411 , Loss: 9.756772859613684e-22\n",
      "Iteration 412 , Loss: 8.584359161811786e-22\n",
      "Iteration 413 , Loss: 7.552909885117068e-22\n",
      "Iteration 414 , Loss: 6.645405870483427e-22\n",
      "Iteration 415 , Loss: 5.846901019634058e-22\n",
      "Iteration 416 , Loss: 5.144351325193175e-22\n",
      "Iteration 417 , Loss: 4.52620716095221e-22\n",
      "Iteration 418 , Loss: 3.982339895859655e-22\n",
      "Iteration 419 , Loss: 3.503868797044045e-22\n",
      "Iteration 420 , Loss: 3.0828227426824873e-22\n",
      "Iteration 421 , Loss: 2.712437239958616e-22\n",
      "Iteration 422 , Loss: 2.386472587529711e-22\n",
      "Iteration 423 , Loss: 2.0997798218891782e-22\n",
      "Iteration 424 , Loss: 1.8474856870947254e-22\n",
      "Iteration 425 , Loss: 1.6254893805400838e-22\n",
      "Iteration 426 , Loss: 1.4301842216520468e-22\n",
      "Iteration 427 , Loss: 1.258391369726838e-22\n",
      "Iteration 428 , Loss: 1.107174475018788e-22\n",
      "Iteration 429 , Loss: 9.741540612755534e-23\n",
      "Iteration 430 , Loss: 8.57125668686408e-23\n",
      "Iteration 431 , Loss: 7.541447200526497e-23\n",
      "Iteration 432 , Loss: 6.635140120286116e-23\n",
      "Iteration 433 , Loss: 5.837744229719042e-23\n",
      "Iteration 434 , Loss: 5.136175614027851e-23\n",
      "Iteration 435 , Loss: 4.5190480743662997e-23\n",
      "Iteration 436 , Loss: 3.9760527485374946e-23\n",
      "Iteration 437 , Loss: 3.498268265429178e-23\n",
      "Iteration 438 , Loss: 3.077991414653011e-23\n",
      "Iteration 439 , Loss: 2.7080645462300823e-23\n",
      "Iteration 440 , Loss: 2.3826041903911722e-23\n",
      "Iteration 441 , Loss: 2.0964226073683353e-23\n",
      "Iteration 442 , Loss: 1.8444716400715326e-23\n",
      "Iteration 443 , Loss: 1.622936935558396e-23\n",
      "Iteration 444 , Loss: 1.427857048160389e-23\n",
      "Iteration 445 , Loss: 1.256218737142964e-23\n",
      "Iteration 446 , Loss: 1.105234902770825e-23\n",
      "Iteration 447 , Loss: 9.724816599805993e-24\n",
      "Iteration 448 , Loss: 8.556561693303652e-24\n",
      "Iteration 449 , Loss: 7.527705342916273e-24\n",
      "Iteration 450 , Loss: 6.623672401254966e-24\n",
      "Iteration 451 , Loss: 5.828264913292999e-24\n",
      "Iteration 452 , Loss: 5.1278080412977625e-24\n",
      "Iteration 453 , Loss: 4.5120640214340425e-24\n",
      "Iteration 454 , Loss: 3.970346257478283e-24\n",
      "Iteration 455 , Loss: 3.493231381635253e-24\n",
      "Iteration 456 , Loss: 3.0735460661822165e-24\n",
      "Iteration 457 , Loss: 2.703663074641948e-24\n",
      "Iteration 458 , Loss: 2.3790134694813395e-24\n",
      "Iteration 459 , Loss: 2.093353641391591e-24\n",
      "Iteration 460 , Loss: 1.8414808223615486e-24\n",
      "Iteration 461 , Loss: 1.620023171666963e-24\n",
      "Iteration 462 , Loss: 1.425610572290885e-24\n",
      "Iteration 463 , Loss: 1.2541356153965212e-24\n",
      "Iteration 464 , Loss: 1.1037397061725721e-24\n",
      "Iteration 465 , Loss: 9.710800829332396e-25\n",
      "Iteration 466 , Loss: 8.543090899567152e-25\n",
      "Iteration 467 , Loss: 7.517627815489467e-25\n",
      "Iteration 468 , Loss: 6.613566391827992e-25\n",
      "Iteration 469 , Loss: 5.818102183495826e-25\n",
      "Iteration 470 , Loss: 5.119361087050933e-25\n",
      "Iteration 471 , Loss: 4.50228976441779e-25\n",
      "Iteration 472 , Loss: 3.9621337243427967e-25\n",
      "Iteration 473 , Loss: 3.486245423399761e-25\n",
      "Iteration 474 , Loss: 3.0662534218994154e-25\n",
      "Iteration 475 , Loss: 2.696939869102839e-25\n",
      "Iteration 476 , Loss: 2.3735762218105723e-25\n",
      "Iteration 477 , Loss: 2.088703041347353e-25\n",
      "Iteration 478 , Loss: 1.8377071380877398e-25\n",
      "Iteration 479 , Loss: 1.6156882375109927e-25\n",
      "Iteration 480 , Loss: 1.4217386257231626e-25\n",
      "Iteration 481 , Loss: 1.2515502901800066e-25\n",
      "Iteration 482 , Loss: 1.1020445413467144e-25\n",
      "Iteration 483 , Loss: 9.69204001063231e-26\n",
      "Iteration 484 , Loss: 8.52897952612371e-26\n",
      "Iteration 485 , Loss: 7.503204354728186e-26\n",
      "Iteration 486 , Loss: 6.607895086661456e-26\n",
      "Iteration 487 , Loss: 5.808180160274963e-26\n",
      "Iteration 488 , Loss: 5.11135415078887e-26\n",
      "Iteration 489 , Loss: 4.4930997274649596e-26\n",
      "Iteration 490 , Loss: 3.9510130986210436e-26\n",
      "Iteration 491 , Loss: 3.4737016182640936e-26\n",
      "Iteration 492 , Loss: 3.0590625560808274e-26\n",
      "Iteration 493 , Loss: 2.689852444981564e-26\n",
      "Iteration 494 , Loss: 2.3652485084347617e-26\n",
      "Iteration 495 , Loss: 2.0814837622842952e-26\n",
      "Iteration 496 , Loss: 1.8282092027147003e-26\n",
      "Iteration 497 , Loss: 1.6117318843671556e-26\n",
      "Iteration 498 , Loss: 1.422430920500972e-26\n",
      "Iteration 499 , Loss: 1.2502222189645357e-26\n",
      "Iteration 500 , Loss: 1.101029516562194e-26\n",
      "Iteration 501 , Loss: 9.681283903745201e-27\n",
      "Iteration 502 , Loss: 8.522067440590502e-27\n",
      "Iteration 503 , Loss: 7.496705612279643e-27\n",
      "Iteration 504 , Loss: 6.611309394526165e-27\n",
      "Iteration 505 , Loss: 5.8153502819021196e-27\n",
      "Iteration 506 , Loss: 5.13585735302602e-27\n",
      "Iteration 507 , Loss: 4.5302687116848755e-27\n",
      "Iteration 508 , Loss: 3.9766478232191205e-27\n",
      "Iteration 509 , Loss: 3.500025036150984e-27\n",
      "Iteration 510 , Loss: 3.0660997307639234e-27\n",
      "Iteration 511 , Loss: 2.696795730579871e-27\n",
      "Iteration 512 , Loss: 2.3849485762057065e-27\n",
      "Iteration 513 , Loss: 2.0922563358724286e-27\n",
      "Iteration 514 , Loss: 1.8377071380877398e-27\n",
      "Iteration 515 , Loss: 1.6185902364549152e-27\n",
      "Iteration 516 , Loss: 1.4133785490205664e-27\n",
      "Iteration 517 , Loss: 1.246449534055775e-27\n",
      "Iteration 518 , Loss: 1.090007207638172e-27\n",
      "Iteration 519 , Loss: 9.577457020443285e-28\n",
      "Iteration 520 , Loss: 8.477180946934635e-28\n",
      "Iteration 521 , Loss: 7.444019680127991e-28\n",
      "Iteration 522 , Loss: 6.591495234665315e-28\n",
      "Iteration 523 , Loss: 5.790801415865988e-28\n",
      "Iteration 524 , Loss: 5.148444075584569e-28\n",
      "Iteration 525 , Loss: 4.543838814073028e-28\n",
      "Iteration 526 , Loss: 4.021389872129158e-28\n",
      "Iteration 527 , Loss: 3.5308381819242297e-28\n",
      "Iteration 528 , Loss: 3.1112261952908607e-28\n",
      "Iteration 529 , Loss: 2.7548810073306124e-28\n",
      "Iteration 530 , Loss: 2.4592199459880615e-28\n",
      "Iteration 531 , Loss: 2.180337586321012e-28\n",
      "Iteration 532 , Loss: 1.9491104371978806e-28\n",
      "Iteration 533 , Loss: 1.7308409447405864e-28\n",
      "Iteration 534 , Loss: 1.5255291089491295e-28\n",
      "Iteration 535 , Loss: 1.33317492982351e-28\n",
      "Iteration 536 , Loss: 1.1537784073637277e-28\n",
      "Iteration 537 , Loss: 9.873395415697828e-29\n",
      "Iteration 538 , Loss: 8.775018309114343e-29\n",
      "Iteration 539 , Loss: 7.937989895984207e-29\n",
      "Iteration 540 , Loss: 6.95647821842614e-29\n",
      "Iteration 541 , Loss: 6.213512223779876e-29\n",
      "Iteration 542 , Loss: 5.512492983322366e-29\n",
      "Iteration 543 , Loss: 4.85342049705361e-29\n",
      "Iteration 544 , Loss: 4.0930055771111975e-29\n",
      "Iteration 545 , Loss: 3.66111578708236e-29\n",
      "Iteration 546 , Loss: 3.2533001213583627e-29\n",
      "Iteration 547 , Loss: 2.869558579939206e-29\n",
      "Iteration 548 , Loss: 2.622365471577104e-29\n",
      "Iteration 549 , Loss: 2.2790684589900794e-29\n",
      "Iteration 550 , Loss: 1.959845570707895e-29\n",
      "Iteration 551 , Loss: 1.6646968067305512e-29\n",
      "Iteration 552 , Loss: 1.4777467870288822e-29\n",
      "Iteration 553 , Loss: 1.2230425518836703e-29\n",
      "Iteration 554 , Loss: 9.924124410432988e-30\n",
      "Iteration 555 , Loss: 9.924124410432988e-30\n",
      "Iteration 556 , Loss: 9.236952606275622e-30\n",
      "Iteration 557 , Loss: 9.236952606275622e-30\n",
      "Iteration 558 , Loss: 8.574432705406413e-30\n",
      "Iteration 559 , Loss: 8.574432705406413e-30\n",
      "Iteration 560 , Loss: 7.93656470782536e-30\n",
      "Iteration 561 , Loss: 7.93656470782536e-30\n",
      "Iteration 562 , Loss: 7.93656470782536e-30\n",
      "Iteration 563 , Loss: 7.323348613532464e-30\n",
      "Iteration 564 , Loss: 7.323348613532464e-30\n",
      "Iteration 565 , Loss: 6.734784422527725e-30\n",
      "Iteration 566 , Loss: 6.734784422527725e-30\n",
      "Iteration 567 , Loss: 6.1708721348111424e-30\n",
      "Iteration 568 , Loss: 6.1708721348111424e-30\n",
      "Iteration 569 , Loss: 6.1708721348111424e-30\n",
      "Iteration 570 , Loss: 5.6316117503827164e-30\n",
      "Iteration 571 , Loss: 5.6316117503827164e-30\n",
      "Iteration 572 , Loss: 5.117003269242447e-30\n",
      "Iteration 573 , Loss: 5.117003269242447e-30\n",
      "Iteration 574 , Loss: 5.117003269242447e-30\n",
      "Iteration 575 , Loss: 4.627046691390334e-30\n",
      "Iteration 576 , Loss: 4.627046691390334e-30\n",
      "Iteration 577 , Loss: 4.627046691390334e-30\n",
      "Iteration 578 , Loss: 4.161742016826378e-30\n",
      "Iteration 579 , Loss: 4.161742016826378e-30\n",
      "Iteration 580 , Loss: 4.161742016826378e-30\n",
      "Iteration 581 , Loss: 3.7210892455505784e-30\n",
      "Iteration 582 , Loss: 3.7210892455505784e-30\n",
      "Iteration 583 , Loss: 3.7210892455505784e-30\n",
      "Iteration 584 , Loss: 3.3050883775629354e-30\n",
      "Iteration 585 , Loss: 3.3050883775629354e-30\n",
      "Iteration 586 , Loss: 3.3050883775629354e-30\n",
      "Iteration 587 , Loss: 2.913739412863449e-30\n",
      "Iteration 588 , Loss: 2.913739412863449e-30\n",
      "Iteration 589 , Loss: 2.913739412863449e-30\n",
      "Iteration 590 , Loss: 2.913739412863449e-30\n",
      "Iteration 591 , Loss: 2.5470423514521194e-30\n",
      "Iteration 592 , Loss: 2.5470423514521194e-30\n",
      "Iteration 593 , Loss: 2.5470423514521194e-30\n",
      "Iteration 594 , Loss: 2.5470423514521194e-30\n",
      "Iteration 595 , Loss: 2.2049971933289463e-30\n",
      "Iteration 596 , Loss: 2.2049971933289463e-30\n",
      "Iteration 597 , Loss: 2.2049971933289463e-30\n",
      "Iteration 598 , Loss: 2.2049971933289463e-30\n",
      "Iteration 599 , Loss: 1.88760393849393e-30\n",
      "Iteration 600 , Loss: 1.88760393849393e-30\n",
      "Iteration 601 , Loss: 1.88760393849393e-30\n",
      "Iteration 602 , Loss: 1.88760393849393e-30\n",
      "Iteration 603 , Loss: 1.59486258694707e-30\n",
      "Iteration 604 , Loss: 1.59486258694707e-30\n",
      "Iteration 605 , Loss: 1.59486258694707e-30\n",
      "Iteration 606 , Loss: 1.59486258694707e-30\n",
      "Iteration 607 , Loss: 1.59486258694707e-30\n",
      "Iteration 608 , Loss: 1.3267731386883668e-30\n",
      "Iteration 609 , Loss: 1.3267731386883668e-30\n",
      "Iteration 610 , Loss: 1.3267731386883668e-30\n",
      "Iteration 611 , Loss: 1.3267731386883668e-30\n",
      "Iteration 612 , Loss: 1.3267731386883668e-30\n",
      "Iteration 613 , Loss: 1.0833355937178202e-30\n",
      "Iteration 614 , Loss: 1.0833355937178202e-30\n",
      "Iteration 615 , Loss: 1.0833355937178202e-30\n",
      "Iteration 616 , Loss: 1.0833355937178202e-30\n",
      "Iteration 617 , Loss: 1.0833355937178202e-30\n",
      "Iteration 618 , Loss: 1.0833355937178202e-30\n",
      "Iteration 619 , Loss: 8.645499520354302e-31\n",
      "Iteration 620 , Loss: 8.645499520354302e-31\n",
      "Iteration 621 , Loss: 8.645499520354302e-31\n",
      "Iteration 622 , Loss: 8.645499520354302e-31\n",
      "Iteration 623 , Loss: 8.645499520354302e-31\n",
      "Iteration 624 , Loss: 8.645499520354302e-31\n",
      "Iteration 625 , Loss: 8.645499520354302e-31\n",
      "Iteration 626 , Loss: 6.704162136411968e-31\n",
      "Iteration 627 , Loss: 6.704162136411968e-31\n",
      "Iteration 628 , Loss: 6.704162136411968e-31\n",
      "Iteration 629 , Loss: 6.704162136411968e-31\n",
      "Iteration 630 , Loss: 6.704162136411968e-31\n",
      "Iteration 631 , Loss: 6.704162136411968e-31\n",
      "Iteration 632 , Loss: 6.704162136411968e-31\n",
      "Iteration 633 , Loss: 5.0093437853512005e-31\n",
      "Iteration 634 , Loss: 5.0093437853512005e-31\n",
      "Iteration 635 , Loss: 5.0093437853512005e-31\n",
      "Iteration 636 , Loss: 5.0093437853512005e-31\n",
      "Iteration 637 , Loss: 5.0093437853512005e-31\n",
      "Iteration 638 , Loss: 5.0093437853512005e-31\n",
      "Iteration 639 , Loss: 5.0093437853512005e-31\n",
      "Iteration 640 , Loss: 5.0093437853512005e-31\n",
      "Iteration 641 , Loss: 5.0093437853512005e-31\n",
      "Iteration 642 , Loss: 3.561044467171999e-31\n",
      "Iteration 643 , Loss: 3.561044467171999e-31\n",
      "Iteration 644 , Loss: 3.561044467171999e-31\n",
      "Iteration 645 , Loss: 3.561044467171999e-31\n",
      "Iteration 646 , Loss: 3.561044467171999e-31\n",
      "Iteration 647 , Loss: 3.561044467171999e-31\n",
      "Iteration 648 , Loss: 3.561044467171999e-31\n",
      "Iteration 649 , Loss: 3.561044467171999e-31\n",
      "Iteration 650 , Loss: 3.561044467171999e-31\n",
      "Iteration 651 , Loss: 3.561044467171999e-31\n",
      "Iteration 652 , Loss: 2.359264181874364e-31\n",
      "Iteration 653 , Loss: 2.359264181874364e-31\n",
      "Iteration 654 , Loss: 2.359264181874364e-31\n",
      "Iteration 655 , Loss: 2.359264181874364e-31\n",
      "Iteration 656 , Loss: 2.359264181874364e-31\n",
      "Iteration 657 , Loss: 2.359264181874364e-31\n",
      "Iteration 658 , Loss: 2.359264181874364e-31\n",
      "Iteration 659 , Loss: 2.359264181874364e-31\n",
      "Iteration 660 , Loss: 2.359264181874364e-31\n",
      "Iteration 661 , Loss: 2.359264181874364e-31\n",
      "Iteration 662 , Loss: 2.359264181874364e-31\n",
      "Iteration 663 , Loss: 2.359264181874364e-31\n",
      "Iteration 664 , Loss: 2.359264181874364e-31\n",
      "Iteration 665 , Loss: 2.359264181874364e-31\n",
      "Iteration 666 , Loss: 1.404002929458295e-31\n",
      "Iteration 667 , Loss: 1.404002929458295e-31\n",
      "Iteration 668 , Loss: 1.404002929458295e-31\n",
      "Iteration 669 , Loss: 1.404002929458295e-31\n",
      "Iteration 670 , Loss: 1.404002929458295e-31\n",
      "Iteration 671 , Loss: 1.404002929458295e-31\n",
      "Iteration 672 , Loss: 1.404002929458295e-31\n",
      "Iteration 673 , Loss: 1.404002929458295e-31\n",
      "Iteration 674 , Loss: 1.404002929458295e-31\n",
      "Iteration 675 , Loss: 1.404002929458295e-31\n",
      "Iteration 676 , Loss: 1.404002929458295e-31\n",
      "Iteration 677 , Loss: 1.404002929458295e-31\n",
      "Iteration 678 , Loss: 1.404002929458295e-31\n",
      "Iteration 679 , Loss: 1.404002929458295e-31\n",
      "Iteration 680 , Loss: 6.952607099237921e-32\n",
      "Iteration 681 , Loss: 6.952607099237921e-32\n",
      "Iteration 682 , Loss: 6.952607099237921e-32\n",
      "Iteration 683 , Loss: 6.952607099237921e-32\n",
      "Iteration 684 , Loss: 6.952607099237921e-32\n",
      "Iteration 685 , Loss: 6.952607099237921e-32\n",
      "Iteration 686 , Loss: 6.952607099237921e-32\n",
      "Iteration 687 , Loss: 6.952607099237921e-32\n",
      "Iteration 688 , Loss: 6.952607099237921e-32\n",
      "Iteration 689 , Loss: 6.952607099237921e-32\n",
      "Iteration 690 , Loss: 6.952607099237921e-32\n",
      "Iteration 691 , Loss: 6.952607099237921e-32\n",
      "Iteration 692 , Loss: 6.952607099237921e-32\n",
      "Iteration 693 , Loss: 6.952607099237921e-32\n",
      "Iteration 694 , Loss: 6.952607099237921e-32\n",
      "Iteration 695 , Loss: 6.952607099237921e-32\n",
      "Iteration 696 , Loss: 6.952607099237921e-32\n",
      "Iteration 697 , Loss: 6.952607099237921e-32\n",
      "Iteration 698 , Loss: 6.952607099237921e-32\n",
      "Iteration 699 , Loss: 6.952607099237921e-32\n",
      "Iteration 700 , Loss: 6.952607099237921e-32\n",
      "Iteration 701 , Loss: 6.952607099237921e-32\n",
      "Iteration 702 , Loss: 2.3303752327085554e-32\n",
      "Iteration 703 , Loss: 2.3303752327085554e-32\n",
      "Iteration 704 , Loss: 2.3303752327085554e-32\n",
      "Iteration 705 , Loss: 2.3303752327085554e-32\n",
      "Iteration 706 , Loss: 2.3303752327085554e-32\n",
      "Iteration 707 , Loss: 2.3303752327085554e-32\n",
      "Iteration 708 , Loss: 2.3303752327085554e-32\n",
      "Iteration 709 , Loss: 2.3303752327085554e-32\n",
      "Iteration 710 , Loss: 2.3303752327085554e-32\n",
      "Iteration 711 , Loss: 2.3303752327085554e-32\n",
      "Iteration 712 , Loss: 2.3303752327085554e-32\n",
      "Iteration 713 , Loss: 2.3303752327085554e-32\n",
      "Iteration 714 , Loss: 2.3303752327085554e-32\n",
      "Iteration 715 , Loss: 2.3303752327085554e-32\n",
      "Iteration 716 , Loss: 2.3303752327085554e-32\n",
      "Iteration 717 , Loss: 2.3303752327085554e-32\n",
      "Iteration 718 , Loss: 2.3303752327085554e-32\n",
      "Iteration 719 , Loss: 2.3303752327085554e-32\n",
      "Iteration 720 , Loss: 2.3303752327085554e-32\n",
      "Iteration 721 , Loss: 2.3303752327085554e-32\n",
      "Iteration 722 , Loss: 2.3303752327085554e-32\n",
      "Iteration 723 , Loss: 2.3303752327085554e-32\n",
      "Iteration 724 , Loss: 2.3303752327085554e-32\n",
      "Iteration 725 , Loss: 2.3303752327085554e-32\n",
      "Iteration 726 , Loss: 2.3303752327085554e-32\n",
      "Iteration 727 , Loss: 2.3303752327085554e-32\n",
      "Iteration 728 , Loss: 2.3303752327085554e-32\n",
      "Iteration 729 , Loss: 2.3303752327085554e-32\n",
      "Iteration 730 , Loss: 2.3303752327085554e-32\n",
      "Iteration 731 , Loss: 2.3303752327085554e-32\n",
      "Iteration 732 , Loss: 2.3303752327085554e-32\n",
      "Iteration 733 , Loss: 2.3303752327085554e-32\n",
      "Iteration 734 , Loss: 2.3303752327085554e-32\n",
      "Iteration 735 , Loss: 2.3303752327085554e-32\n",
      "Iteration 736 , Loss: 2.3303752327085554e-32\n",
      "Iteration 737 , Loss: 2.3303752327085554e-32\n",
      "Iteration 738 , Loss: 2.3303752327085554e-32\n",
      "Iteration 739 , Loss: 2.3303752327085554e-32\n",
      "Iteration 740 , Loss: 2.3303752327085554e-32\n",
      "Iteration 741 , Loss: 2.3303752327085554e-32\n",
      "Iteration 742 , Loss: 2.3303752327085554e-32\n",
      "Iteration 743 , Loss: 1.7333369499485123e-33\n",
      "Iteration 744 , Loss: 1.7333369499485123e-33\n",
      "Iteration 745 , Loss: 1.7333369499485123e-33\n",
      "Iteration 746 , Loss: 1.7333369499485123e-33\n",
      "Iteration 747 , Loss: 1.7333369499485123e-33\n",
      "Iteration 748 , Loss: 1.7333369499485123e-33\n",
      "Iteration 749 , Loss: 1.7333369499485123e-33\n",
      "Iteration 750 , Loss: 1.7333369499485123e-33\n",
      "Iteration 751 , Loss: 1.7333369499485123e-33\n",
      "Iteration 752 , Loss: 1.7333369499485123e-33\n",
      "Iteration 753 , Loss: 1.7333369499485123e-33\n",
      "Iteration 754 , Loss: 1.7333369499485123e-33\n",
      "Iteration 755 , Loss: 1.7333369499485123e-33\n",
      "Iteration 756 , Loss: 1.7333369499485123e-33\n",
      "Iteration 757 , Loss: 1.7333369499485123e-33\n",
      "Iteration 758 , Loss: 1.7333369499485123e-33\n",
      "Iteration 759 , Loss: 1.7333369499485123e-33\n",
      "Iteration 760 , Loss: 1.7333369499485123e-33\n",
      "Iteration 761 , Loss: 1.7333369499485123e-33\n",
      "Iteration 762 , Loss: 1.7333369499485123e-33\n",
      "Iteration 763 , Loss: 1.7333369499485123e-33\n",
      "Iteration 764 , Loss: 1.7333369499485123e-33\n",
      "Iteration 765 , Loss: 1.7333369499485123e-33\n",
      "Iteration 766 , Loss: 1.7333369499485123e-33\n",
      "Iteration 767 , Loss: 1.7333369499485123e-33\n",
      "Iteration 768 , Loss: 1.7333369499485123e-33\n",
      "Iteration 769 , Loss: 1.7333369499485123e-33\n",
      "Iteration 770 , Loss: 1.7333369499485123e-33\n",
      "Iteration 771 , Loss: 1.7333369499485123e-33\n",
      "Iteration 772 , Loss: 1.7333369499485123e-33\n",
      "Iteration 773 , Loss: 1.7333369499485123e-33\n",
      "Iteration 774 , Loss: 1.7333369499485123e-33\n",
      "Iteration 775 , Loss: 1.7333369499485123e-33\n",
      "Iteration 776 , Loss: 1.7333369499485123e-33\n",
      "Iteration 777 , Loss: 1.7333369499485123e-33\n",
      "Iteration 778 , Loss: 1.7333369499485123e-33\n",
      "Iteration 779 , Loss: 1.7333369499485123e-33\n",
      "Iteration 780 , Loss: 1.7333369499485123e-33\n",
      "Iteration 781 , Loss: 1.7333369499485123e-33\n",
      "Iteration 782 , Loss: 1.7333369499485123e-33\n",
      "Iteration 783 , Loss: 1.7333369499485123e-33\n",
      "Iteration 784 , Loss: 1.7333369499485123e-33\n",
      "Iteration 785 , Loss: 1.7333369499485123e-33\n",
      "Iteration 786 , Loss: 1.7333369499485123e-33\n",
      "Iteration 787 , Loss: 1.7333369499485123e-33\n",
      "Iteration 788 , Loss: 1.7333369499485123e-33\n",
      "Iteration 789 , Loss: 1.7333369499485123e-33\n",
      "Iteration 790 , Loss: 1.7333369499485123e-33\n",
      "Iteration 791 , Loss: 1.7333369499485123e-33\n",
      "Iteration 792 , Loss: 1.7333369499485123e-33\n",
      "Iteration 793 , Loss: 1.7333369499485123e-33\n",
      "Iteration 794 , Loss: 1.7333369499485123e-33\n",
      "Iteration 795 , Loss: 1.7333369499485123e-33\n",
      "Iteration 796 , Loss: 1.7333369499485123e-33\n",
      "Iteration 797 , Loss: 1.7333369499485123e-33\n",
      "Iteration 798 , Loss: 1.7333369499485123e-33\n",
      "Iteration 799 , Loss: 1.7333369499485123e-33\n",
      "Iteration 800 , Loss: 1.7333369499485123e-33\n",
      "Iteration 801 , Loss: 1.7333369499485123e-33\n",
      "Iteration 802 , Loss: 1.7333369499485123e-33\n",
      "Iteration 803 , Loss: 1.7333369499485123e-33\n",
      "Iteration 804 , Loss: 1.7333369499485123e-33\n",
      "Iteration 805 , Loss: 1.7333369499485123e-33\n",
      "Iteration 806 , Loss: 1.7333369499485123e-33\n",
      "Iteration 807 , Loss: 1.7333369499485123e-33\n",
      "Iteration 808 , Loss: 1.7333369499485123e-33\n",
      "Iteration 809 , Loss: 1.7333369499485123e-33\n",
      "Iteration 810 , Loss: 1.7333369499485123e-33\n",
      "Iteration 811 , Loss: 1.7333369499485123e-33\n",
      "Iteration 812 , Loss: 1.7333369499485123e-33\n",
      "Iteration 813 , Loss: 1.7333369499485123e-33\n",
      "Iteration 814 , Loss: 1.7333369499485123e-33\n",
      "Iteration 815 , Loss: 1.7333369499485123e-33\n",
      "Iteration 816 , Loss: 1.7333369499485123e-33\n",
      "Iteration 817 , Loss: 1.7333369499485123e-33\n",
      "Iteration 818 , Loss: 1.7333369499485123e-33\n",
      "Iteration 819 , Loss: 1.7333369499485123e-33\n",
      "Iteration 820 , Loss: 1.7333369499485123e-33\n",
      "Iteration 821 , Loss: 1.7333369499485123e-33\n",
      "Iteration 822 , Loss: 1.7333369499485123e-33\n",
      "Iteration 823 , Loss: 1.7333369499485123e-33\n",
      "Iteration 824 , Loss: 1.7333369499485123e-33\n",
      "Iteration 825 , Loss: 1.7333369499485123e-33\n",
      "Iteration 826 , Loss: 1.7333369499485123e-33\n",
      "Iteration 827 , Loss: 1.7333369499485123e-33\n",
      "Iteration 828 , Loss: 1.7333369499485123e-33\n",
      "Iteration 829 , Loss: 1.7333369499485123e-33\n",
      "Iteration 830 , Loss: 1.7333369499485123e-33\n",
      "Iteration 831 , Loss: 1.7333369499485123e-33\n",
      "Iteration 832 , Loss: 1.7333369499485123e-33\n",
      "Iteration 833 , Loss: 1.7333369499485123e-33\n",
      "Iteration 834 , Loss: 1.7333369499485123e-33\n",
      "Iteration 835 , Loss: 1.7333369499485123e-33\n",
      "Iteration 836 , Loss: 1.7333369499485123e-33\n",
      "Iteration 837 , Loss: 1.7333369499485123e-33\n",
      "Iteration 838 , Loss: 1.7333369499485123e-33\n",
      "Iteration 839 , Loss: 1.7333369499485123e-33\n",
      "Iteration 840 , Loss: 1.7333369499485123e-33\n",
      "Iteration 841 , Loss: 1.7333369499485123e-33\n",
      "Iteration 842 , Loss: 1.7333369499485123e-33\n",
      "Iteration 843 , Loss: 1.7333369499485123e-33\n",
      "Iteration 844 , Loss: 1.7333369499485123e-33\n",
      "Iteration 845 , Loss: 1.7333369499485123e-33\n",
      "Iteration 846 , Loss: 1.7333369499485123e-33\n",
      "Iteration 847 , Loss: 1.7333369499485123e-33\n",
      "Iteration 848 , Loss: 1.7333369499485123e-33\n",
      "Iteration 849 , Loss: 1.7333369499485123e-33\n",
      "Iteration 850 , Loss: 1.7333369499485123e-33\n",
      "Iteration 851 , Loss: 1.7333369499485123e-33\n",
      "Iteration 852 , Loss: 1.7333369499485123e-33\n",
      "Iteration 853 , Loss: 1.7333369499485123e-33\n",
      "Iteration 854 , Loss: 1.7333369499485123e-33\n",
      "Iteration 855 , Loss: 1.7333369499485123e-33\n",
      "Iteration 856 , Loss: 1.7333369499485123e-33\n",
      "Iteration 857 , Loss: 1.7333369499485123e-33\n",
      "Iteration 858 , Loss: 1.7333369499485123e-33\n",
      "Iteration 859 , Loss: 1.7333369499485123e-33\n",
      "Iteration 860 , Loss: 1.7333369499485123e-33\n",
      "Iteration 861 , Loss: 1.7333369499485123e-33\n",
      "Iteration 862 , Loss: 1.7333369499485123e-33\n",
      "Iteration 863 , Loss: 1.7333369499485123e-33\n",
      "Iteration 864 , Loss: 1.7333369499485123e-33\n",
      "Iteration 865 , Loss: 1.7333369499485123e-33\n",
      "Iteration 866 , Loss: 1.7333369499485123e-33\n",
      "Iteration 867 , Loss: 1.7333369499485123e-33\n",
      "Iteration 868 , Loss: 1.7333369499485123e-33\n",
      "Iteration 869 , Loss: 1.7333369499485123e-33\n",
      "Iteration 870 , Loss: 1.7333369499485123e-33\n",
      "Iteration 871 , Loss: 1.7333369499485123e-33\n",
      "Iteration 872 , Loss: 1.7333369499485123e-33\n",
      "Iteration 873 , Loss: 1.7333369499485123e-33\n",
      "Iteration 874 , Loss: 1.7333369499485123e-33\n",
      "Iteration 875 , Loss: 1.7333369499485123e-33\n",
      "Iteration 876 , Loss: 1.7333369499485123e-33\n",
      "Iteration 877 , Loss: 1.7333369499485123e-33\n",
      "Iteration 878 , Loss: 1.7333369499485123e-33\n",
      "Iteration 879 , Loss: 1.7333369499485123e-33\n",
      "Iteration 880 , Loss: 1.7333369499485123e-33\n",
      "Iteration 881 , Loss: 1.7333369499485123e-33\n",
      "Iteration 882 , Loss: 1.7333369499485123e-33\n",
      "Iteration 883 , Loss: 1.7333369499485123e-33\n",
      "Iteration 884 , Loss: 1.7333369499485123e-33\n",
      "Iteration 885 , Loss: 1.7333369499485123e-33\n",
      "Iteration 886 , Loss: 1.7333369499485123e-33\n",
      "Iteration 887 , Loss: 1.7333369499485123e-33\n",
      "Iteration 888 , Loss: 1.7333369499485123e-33\n",
      "Iteration 889 , Loss: 1.7333369499485123e-33\n",
      "Iteration 890 , Loss: 1.7333369499485123e-33\n",
      "Iteration 891 , Loss: 1.7333369499485123e-33\n",
      "Iteration 892 , Loss: 1.7333369499485123e-33\n",
      "Iteration 893 , Loss: 1.7333369499485123e-33\n",
      "Iteration 894 , Loss: 1.7333369499485123e-33\n",
      "Iteration 895 , Loss: 1.7333369499485123e-33\n",
      "Iteration 896 , Loss: 1.7333369499485123e-33\n",
      "Iteration 897 , Loss: 1.7333369499485123e-33\n",
      "Iteration 898 , Loss: 1.7333369499485123e-33\n",
      "Iteration 899 , Loss: 1.7333369499485123e-33\n",
      "Iteration 900 , Loss: 1.7333369499485123e-33\n",
      "Iteration 901 , Loss: 1.7333369499485123e-33\n",
      "Iteration 902 , Loss: 1.7333369499485123e-33\n",
      "Iteration 903 , Loss: 1.7333369499485123e-33\n",
      "Iteration 904 , Loss: 1.7333369499485123e-33\n",
      "Iteration 905 , Loss: 1.7333369499485123e-33\n",
      "Iteration 906 , Loss: 1.7333369499485123e-33\n",
      "Iteration 907 , Loss: 1.7333369499485123e-33\n",
      "Iteration 908 , Loss: 1.7333369499485123e-33\n",
      "Iteration 909 , Loss: 1.7333369499485123e-33\n",
      "Iteration 910 , Loss: 1.7333369499485123e-33\n",
      "Iteration 911 , Loss: 1.7333369499485123e-33\n",
      "Iteration 912 , Loss: 1.7333369499485123e-33\n",
      "Iteration 913 , Loss: 1.7333369499485123e-33\n",
      "Iteration 914 , Loss: 1.7333369499485123e-33\n",
      "Iteration 915 , Loss: 1.7333369499485123e-33\n",
      "Iteration 916 , Loss: 1.7333369499485123e-33\n",
      "Iteration 917 , Loss: 1.7333369499485123e-33\n",
      "Iteration 918 , Loss: 1.7333369499485123e-33\n",
      "Iteration 919 , Loss: 1.7333369499485123e-33\n",
      "Iteration 920 , Loss: 1.7333369499485123e-33\n",
      "Iteration 921 , Loss: 1.7333369499485123e-33\n",
      "Iteration 922 , Loss: 1.7333369499485123e-33\n",
      "Iteration 923 , Loss: 1.7333369499485123e-33\n",
      "Iteration 924 , Loss: 1.7333369499485123e-33\n",
      "Iteration 925 , Loss: 1.7333369499485123e-33\n",
      "Iteration 926 , Loss: 1.7333369499485123e-33\n",
      "Iteration 927 , Loss: 1.7333369499485123e-33\n",
      "Iteration 928 , Loss: 1.7333369499485123e-33\n",
      "Iteration 929 , Loss: 1.7333369499485123e-33\n",
      "Iteration 930 , Loss: 1.7333369499485123e-33\n",
      "Iteration 931 , Loss: 1.7333369499485123e-33\n",
      "Iteration 932 , Loss: 1.7333369499485123e-33\n",
      "Iteration 933 , Loss: 1.7333369499485123e-33\n",
      "Iteration 934 , Loss: 1.7333369499485123e-33\n",
      "Iteration 935 , Loss: 1.7333369499485123e-33\n",
      "Iteration 936 , Loss: 1.7333369499485123e-33\n",
      "Iteration 937 , Loss: 1.7333369499485123e-33\n",
      "Iteration 938 , Loss: 1.7333369499485123e-33\n",
      "Iteration 939 , Loss: 1.7333369499485123e-33\n",
      "Iteration 940 , Loss: 1.7333369499485123e-33\n",
      "Iteration 941 , Loss: 1.7333369499485123e-33\n",
      "Iteration 942 , Loss: 1.7333369499485123e-33\n",
      "Iteration 943 , Loss: 1.7333369499485123e-33\n",
      "Iteration 944 , Loss: 1.7333369499485123e-33\n",
      "Iteration 945 , Loss: 1.7333369499485123e-33\n",
      "Iteration 946 , Loss: 1.7333369499485123e-33\n",
      "Iteration 947 , Loss: 1.7333369499485123e-33\n",
      "Iteration 948 , Loss: 1.7333369499485123e-33\n",
      "Iteration 949 , Loss: 1.7333369499485123e-33\n",
      "Iteration 950 , Loss: 1.7333369499485123e-33\n",
      "Iteration 951 , Loss: 1.7333369499485123e-33\n",
      "Iteration 952 , Loss: 1.7333369499485123e-33\n",
      "Iteration 953 , Loss: 1.7333369499485123e-33\n",
      "Iteration 954 , Loss: 1.7333369499485123e-33\n",
      "Iteration 955 , Loss: 1.7333369499485123e-33\n",
      "Iteration 956 , Loss: 1.7333369499485123e-33\n",
      "Iteration 957 , Loss: 1.7333369499485123e-33\n",
      "Iteration 958 , Loss: 1.7333369499485123e-33\n",
      "Iteration 959 , Loss: 1.7333369499485123e-33\n",
      "Iteration 960 , Loss: 1.7333369499485123e-33\n",
      "Iteration 961 , Loss: 1.7333369499485123e-33\n",
      "Iteration 962 , Loss: 1.7333369499485123e-33\n",
      "Iteration 963 , Loss: 1.7333369499485123e-33\n",
      "Iteration 964 , Loss: 1.7333369499485123e-33\n",
      "Iteration 965 , Loss: 1.7333369499485123e-33\n",
      "Iteration 966 , Loss: 1.7333369499485123e-33\n",
      "Iteration 967 , Loss: 1.7333369499485123e-33\n",
      "Iteration 968 , Loss: 1.7333369499485123e-33\n",
      "Iteration 969 , Loss: 1.7333369499485123e-33\n",
      "Iteration 970 , Loss: 1.7333369499485123e-33\n",
      "Iteration 971 , Loss: 1.7333369499485123e-33\n",
      "Iteration 972 , Loss: 1.7333369499485123e-33\n",
      "Iteration 973 , Loss: 1.7333369499485123e-33\n",
      "Iteration 974 , Loss: 1.7333369499485123e-33\n",
      "Iteration 975 , Loss: 1.7333369499485123e-33\n",
      "Iteration 976 , Loss: 1.7333369499485123e-33\n",
      "Iteration 977 , Loss: 1.7333369499485123e-33\n",
      "Iteration 978 , Loss: 1.7333369499485123e-33\n",
      "Iteration 979 , Loss: 1.7333369499485123e-33\n",
      "Iteration 980 , Loss: 1.7333369499485123e-33\n",
      "Iteration 981 , Loss: 1.7333369499485123e-33\n",
      "Iteration 982 , Loss: 1.7333369499485123e-33\n",
      "Iteration 983 , Loss: 1.7333369499485123e-33\n",
      "Iteration 984 , Loss: 1.7333369499485123e-33\n",
      "Iteration 985 , Loss: 1.7333369499485123e-33\n",
      "Iteration 986 , Loss: 1.7333369499485123e-33\n",
      "Iteration 987 , Loss: 1.7333369499485123e-33\n",
      "Iteration 988 , Loss: 1.7333369499485123e-33\n",
      "Iteration 989 , Loss: 1.7333369499485123e-33\n",
      "Iteration 990 , Loss: 1.7333369499485123e-33\n",
      "Iteration 991 , Loss: 1.7333369499485123e-33\n",
      "Iteration 992 , Loss: 1.7333369499485123e-33\n",
      "Iteration 993 , Loss: 1.7333369499485123e-33\n",
      "Iteration 994 , Loss: 1.7333369499485123e-33\n",
      "Iteration 995 , Loss: 1.7333369499485123e-33\n",
      "Iteration 996 , Loss: 1.7333369499485123e-33\n",
      "Iteration 997 , Loss: 1.7333369499485123e-33\n",
      "Iteration 998 , Loss: 1.7333369499485123e-33\n",
      "Iteration 999 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1000 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1001 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1002 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1003 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1004 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1005 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1006 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1007 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1008 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1009 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1010 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1011 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1012 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1013 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1014 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1015 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1016 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1017 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1018 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1019 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1020 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1021 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1022 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1023 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1024 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1025 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1026 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1027 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1028 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1029 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1030 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1031 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1032 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1033 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1034 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1035 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1036 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1037 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1038 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1039 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1040 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1041 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1042 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1043 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1044 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1045 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1046 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1047 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1048 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1049 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1050 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1051 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1052 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1053 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1054 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1055 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1056 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1057 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1058 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1059 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1060 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1061 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1062 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1063 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1064 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1065 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1066 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1067 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1068 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1069 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1070 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1071 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1072 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1073 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1074 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1075 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1076 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1077 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1078 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1079 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1080 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1081 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1082 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1083 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1084 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1085 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1086 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1087 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1088 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1089 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1090 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1091 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1092 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1093 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1094 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1095 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1096 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1097 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1098 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1099 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1100 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1101 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1102 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1103 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1104 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1105 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1106 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1107 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1108 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1109 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1110 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1111 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1112 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1113 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1114 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1115 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1116 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1117 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1118 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1119 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1120 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1121 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1122 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1123 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1124 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1125 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1126 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1127 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1128 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1129 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1130 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1131 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1132 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1133 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1134 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1135 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1136 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1137 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1138 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1139 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1140 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1141 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1142 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1143 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1144 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1145 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1146 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1147 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1148 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1149 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1150 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1151 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1152 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1153 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1154 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1155 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1156 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1157 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1158 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1159 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1160 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1161 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1162 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1163 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1164 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1165 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1166 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1167 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1168 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1169 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1170 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1171 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1172 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1173 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1174 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1175 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1176 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1177 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1178 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1179 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1180 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1181 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1182 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1183 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1184 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1185 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1186 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1187 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1188 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1189 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1190 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1191 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1192 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1193 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1194 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1195 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1196 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1197 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1198 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1199 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1200 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1201 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1202 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1203 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1204 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1205 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1206 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1207 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1208 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1209 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1210 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1211 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1212 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1213 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1214 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1215 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1216 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1217 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1218 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1219 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1220 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1221 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1222 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1223 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1224 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1225 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1226 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1227 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1228 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1229 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1230 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1231 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1232 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1233 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1234 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1235 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1236 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1237 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1238 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1239 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1240 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1241 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1242 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1243 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1244 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1245 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1246 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1247 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1248 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1249 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1250 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1251 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1252 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1253 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1254 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1255 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1256 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1257 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1258 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1259 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1260 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1261 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1262 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1263 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1264 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1265 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1266 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1267 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1268 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1269 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1270 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1271 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1272 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1273 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1274 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1275 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1276 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1277 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1278 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1279 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1280 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1281 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1282 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1283 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1284 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1285 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1286 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1287 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1288 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1289 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1290 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1291 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1292 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1293 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1294 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1295 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1296 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1297 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1298 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1299 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1300 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1301 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1302 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1303 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1304 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1305 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1306 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1307 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1308 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1309 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1310 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1311 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1312 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1313 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1314 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1315 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1316 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1317 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1318 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1319 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1320 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1321 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1322 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1323 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1324 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1325 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1326 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1327 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1328 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1329 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1330 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1331 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1332 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1333 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1334 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1335 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1336 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1337 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1338 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1339 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1340 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1341 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1342 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1343 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1344 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1345 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1346 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1347 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1348 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1349 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1350 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1351 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1352 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1353 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1354 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1355 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1356 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1357 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1358 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1359 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1360 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1361 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1362 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1363 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1364 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1365 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1366 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1367 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1368 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1369 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1370 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1371 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1372 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1373 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1374 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1375 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1376 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1377 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1378 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1379 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1380 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1381 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1382 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1383 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1384 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1385 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1386 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1387 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1388 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1389 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1390 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1391 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1392 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1393 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1394 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1395 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1396 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1397 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1398 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1399 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1400 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1401 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1402 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1403 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1404 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1405 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1406 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1407 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1408 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1409 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1410 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1411 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1412 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1413 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1414 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1415 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1416 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1417 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1418 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1419 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1420 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1421 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1422 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1423 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1424 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1425 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1426 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1427 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1428 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1429 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1430 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1431 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1432 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1433 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1434 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1435 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1436 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1437 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1438 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1439 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1440 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1441 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1442 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1443 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1444 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1445 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1446 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1447 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1448 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1449 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1450 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1451 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1452 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1453 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1454 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1455 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1456 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1457 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1458 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1459 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1460 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1461 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1462 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1463 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1464 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1465 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1466 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1467 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1468 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1469 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1470 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1471 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1472 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1473 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1474 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1475 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1476 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1477 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1478 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1479 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1480 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1481 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1482 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1483 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1484 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1485 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1486 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1487 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1488 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1489 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1490 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1491 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1492 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1493 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1494 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1495 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1496 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1497 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1498 , Loss: 1.7333369499485123e-33\n",
      "Iteration 1499 , Loss: 1.7333369499485123e-33\n",
      "Final weights:\n",
      "  [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]\n",
      " [ 0.25975286  0.11950572 -0.02074143 -0.16098857]\n",
      " [ 0.53548387  0.27096774  0.00645161 -0.25806452]]\n",
      "Final bias:\n",
      " [-0.00698895 -0.04024714 -0.06451613]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "weight = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "])\n",
    "bias = np.array([0.1, 0.2, 0.3])\n",
    "learning_rate = 0.001\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivatives(x):\n",
    "    return np.where(x>0, 1, 0)\n",
    "\n",
    "for iteration in range(1500):\n",
    "    z = np.dot(weight, input) + bias\n",
    "    a = relu(z)\n",
    "    y = np.sum(a)\n",
    "    loss = y ** 2\n",
    "\n",
    "# Gradient of loss with respect to output y\n",
    "    dL_dY = 2 * y\n",
    "\n",
    "# Gradient of y with respect to a\n",
    "    dY_dA = np.ones_like(a)\n",
    "\n",
    "    dL_dA = dL_dY * dY_dA\n",
    "\n",
    "# Gradient of a with respect to z (ReLU derivative)\n",
    "    dA_dZ = relu_derivatives(z)\n",
    "\n",
    "# Gradient of loss with respect to z\n",
    "    dL_dZ = dL_dA * dA_dZ\n",
    "\n",
    "    dLoss_dW = np.outer( dL_dZ , input)\n",
    "\n",
    "    dL_db = dL_dZ\n",
    "    weight -= learning_rate * dLoss_dW\n",
    "    bias -= learning_rate * dL_db\n",
    "\n",
    "# if iteration % 200 == 0:\n",
    "    print(f\"Iteration {iteration } , Loss: {loss}\")\n",
    "\n",
    "print(\"Final weights:\\n \", weight)\n",
    "print(\"Final bias:\\n\", bias)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
