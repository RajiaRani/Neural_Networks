{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e200a4a1",
   "metadata": {},
   "source": [
    "# ðŸ”— Dataset: Pima Indians Diabetes\n",
    "\n",
    "*Small and ideal for logistic regression / binary classification.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9733efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd032ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1   2   3    4     5      6   7  8\n",
      "0  6  148  72  35    0  33.6  0.627  50  1\n",
      "1  1   85  66  29    0  26.6  0.351  31  0\n",
      "2  8  183  64   0    0  23.3  0.672  32  1\n",
      "3  1   89  66  23   94  28.1  0.167  21  0\n",
      "4  0  137  40  35  168  43.1  2.288  33  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "\n",
    "df = pd.read_csv(url, header=None)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ea4440ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values   # first 8 columns â†’ features\n",
    "y = df.iloc[:, -1].values   # last column â†’ label (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3017b08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,  33.6  ,   0.627,  50.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,  26.6  ,   0.351,  31.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,  23.3  ,   0.672,  32.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,  26.2  ,   0.245,  30.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,  30.1  ,   0.349,  47.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,  30.4  ,   0.315,  23.   ]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "47620d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3302e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0ee1a2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  6.0000, 148.0000,  72.0000,  ...,  33.6000,   0.6270,  50.0000],\n",
       "        [  1.0000,  85.0000,  66.0000,  ...,  26.6000,   0.3510,  31.0000],\n",
       "        [  8.0000, 183.0000,  64.0000,  ...,  23.3000,   0.6720,  32.0000],\n",
       "        ...,\n",
       "        [  5.0000, 121.0000,  72.0000,  ...,  26.2000,   0.2450,  30.0000],\n",
       "        [  1.0000, 126.0000,  60.0000,  ...,  30.1000,   0.3490,  47.0000],\n",
       "        [  1.0000,  93.0000,  70.0000,  ...,  30.4000,   0.3150,  23.0000]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8c375bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize inputs (very important here)\n",
    "# X = (X - X.mean(dim=0))/X.std(dim=0)\n",
    "X = X / X.max(dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9c88718c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3529, 0.7437, 0.5902,  ..., 0.5007, 0.2591, 0.6173],\n",
       "        [0.0588, 0.4271, 0.5410,  ..., 0.3964, 0.1450, 0.3827],\n",
       "        [0.4706, 0.9196, 0.5246,  ..., 0.3472, 0.2777, 0.3951],\n",
       "        ...,\n",
       "        [0.2941, 0.6080, 0.5902,  ..., 0.3905, 0.1012, 0.3704],\n",
       "        [0.0588, 0.6332, 0.4918,  ..., 0.4486, 0.1442, 0.5802],\n",
       "        [0.0588, 0.4673, 0.5738,  ..., 0.4531, 0.1302, 0.2840]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e8848559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 8])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c1193292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_input, n_neurons):\n",
    "        self.weights = 0.01 * (torch.rand(n_input, n_neurons, dtype=torch.float32))\n",
    "        self.weights.requires_grad_()\n",
    "        self.bias = torch.zeros((1, n_neurons), dtype=torch.float32)\n",
    "        self.bias.requires_grad_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.output = torch.matmul(input, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5bd01321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Relu:\n",
    "    def forward(self, input):\n",
    "        self.output = torch.maximum(input, torch.tensor(0.0, device=input.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a9d836a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, input):\n",
    "        self.output = 1 / (1 + torch.exp(-input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2100a224",
   "metadata": {},
   "source": [
    "# Binary Cross-Entropy Loss from scratch\n",
    "\n",
    "*Inputs: y_pred must be probabilities (after sigmoid), y_true must be 0/1 floats.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a40931bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_BinaryCrossEntropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "         # clamp to avoid log(0)\n",
    "          y_pred = torch.clamp(y_pred, 1e-7, 1 - 1e-7)\n",
    "          # BCE formula: -( y*log(p) + (1-y)*log(1-p) )\n",
    "          loss = - (y_true*torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "          return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d3d27560",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = Linear(8, 16)\n",
    "activation1 = Activation_Relu()\n",
    "linear2 = Linear(16, 8)\n",
    "activation2 = Activation_Relu()\n",
    "linear3 = Linear(8,4)\n",
    "activation3 = Activation_Relu()\n",
    "output_layer = Linear(4,1)\n",
    "sigmoid = Sigmoid()\n",
    "LBC = Loss_BinaryCrossEntropy()\n",
    "\n",
    "# linear1.forward(X)\n",
    "# activation1.forward(linear1.output)\n",
    "\n",
    "# linear2.forward(activation1.output)\n",
    "# activation2.forward(linear2.output)\n",
    "\n",
    "# linear3.forward(activation2.output)\n",
    "# activation3.forward(linear3.output)\n",
    "\n",
    "# output_layer.forward(activation3.output)\n",
    "# sigmoid.forward(output_layer.output)\n",
    "# print(sigmoid.output[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d05be1",
   "metadata": {},
   "source": [
    "# Collect parameters (this is REQUIRED)\n",
    "\n",
    "*Because you built everything from scratch, PyTorch does not know what to update unless you tell it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "034a9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    linear1.weights, linear1.bias,\n",
    "    linear2.weights, linear2.bias,\n",
    "    linear3.weights, linear3.bias,\n",
    "    output_layer.weights, output_layer.bias\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cb5706f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer (SGD or Adam)\n",
    "optimizer = torch.optim.SGD(params, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f5a5a26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100  Loss: 0.6472  Acc(last batch): 0.594\n",
      "Epoch 2/100  Loss: 0.6471  Acc(last batch): 0.656\n",
      "Epoch 3/100  Loss: 0.6473  Acc(last batch): 0.844\n",
      "Epoch 4/100  Loss: 0.6471  Acc(last batch): 0.750\n",
      "Epoch 5/100  Loss: 0.6471  Acc(last batch): 0.625\n",
      "Epoch 6/100  Loss: 0.6472  Acc(last batch): 0.781\n",
      "Epoch 7/100  Loss: 0.6470  Acc(last batch): 0.750\n",
      "Epoch 8/100  Loss: 0.6471  Acc(last batch): 0.719\n",
      "Epoch 9/100  Loss: 0.6471  Acc(last batch): 0.500\n",
      "Epoch 10/100  Loss: 0.6471  Acc(last batch): 0.625\n",
      "Epoch 11/100  Loss: 0.6471  Acc(last batch): 0.625\n",
      "Epoch 12/100  Loss: 0.6471  Acc(last batch): 0.594\n",
      "Epoch 13/100  Loss: 0.6472  Acc(last batch): 0.719\n",
      "Epoch 14/100  Loss: 0.6471  Acc(last batch): 0.656\n",
      "Epoch 15/100  Loss: 0.6471  Acc(last batch): 0.750\n",
      "Epoch 16/100  Loss: 0.6474  Acc(last batch): 0.812\n",
      "Epoch 17/100  Loss: 0.6472  Acc(last batch): 0.656\n",
      "Epoch 18/100  Loss: 0.6471  Acc(last batch): 0.656\n",
      "Epoch 19/100  Loss: 0.6472  Acc(last batch): 0.562\n",
      "Epoch 20/100  Loss: 0.6470  Acc(last batch): 0.594\n",
      "Epoch 21/100  Loss: 0.6473  Acc(last batch): 0.719\n",
      "Epoch 22/100  Loss: 0.6470  Acc(last batch): 0.562\n",
      "Epoch 23/100  Loss: 0.6470  Acc(last batch): 0.688\n",
      "Epoch 24/100  Loss: 0.6470  Acc(last batch): 0.781\n",
      "Epoch 25/100  Loss: 0.6472  Acc(last batch): 0.625\n",
      "Epoch 26/100  Loss: 0.6472  Acc(last batch): 0.719\n",
      "Epoch 27/100  Loss: 0.6471  Acc(last batch): 0.625\n",
      "Epoch 28/100  Loss: 0.6470  Acc(last batch): 0.750\n",
      "Epoch 29/100  Loss: 0.6472  Acc(last batch): 0.688\n",
      "Epoch 30/100  Loss: 0.6472  Acc(last batch): 0.688\n",
      "Epoch 31/100  Loss: 0.6471  Acc(last batch): 0.688\n",
      "Epoch 32/100  Loss: 0.6472  Acc(last batch): 0.719\n",
      "Epoch 33/100  Loss: 0.6472  Acc(last batch): 0.625\n",
      "Epoch 34/100  Loss: 0.6471  Acc(last batch): 0.562\n",
      "Epoch 35/100  Loss: 0.6472  Acc(last batch): 0.688\n",
      "Epoch 36/100  Loss: 0.6472  Acc(last batch): 0.656\n",
      "Epoch 37/100  Loss: 0.6471  Acc(last batch): 0.719\n",
      "Epoch 38/100  Loss: 0.6470  Acc(last batch): 0.719\n",
      "Epoch 39/100  Loss: 0.6472  Acc(last batch): 0.625\n",
      "Epoch 40/100  Loss: 0.6472  Acc(last batch): 0.656\n",
      "Epoch 41/100  Loss: 0.6472  Acc(last batch): 0.656\n",
      "Epoch 42/100  Loss: 0.6472  Acc(last batch): 0.719\n",
      "Epoch 43/100  Loss: 0.6472  Acc(last batch): 0.688\n",
      "Epoch 44/100  Loss: 0.6470  Acc(last batch): 0.562\n",
      "Epoch 45/100  Loss: 0.6470  Acc(last batch): 0.656\n",
      "Epoch 46/100  Loss: 0.6472  Acc(last batch): 0.688\n",
      "Epoch 47/100  Loss: 0.6470  Acc(last batch): 0.812\n",
      "Epoch 48/100  Loss: 0.6473  Acc(last batch): 0.469\n",
      "Epoch 49/100  Loss: 0.6471  Acc(last batch): 0.688\n",
      "Epoch 50/100  Loss: 0.6471  Acc(last batch): 0.688\n",
      "Epoch 51/100  Loss: 0.6471  Acc(last batch): 0.469\n",
      "Epoch 52/100  Loss: 0.6472  Acc(last batch): 0.688\n",
      "Epoch 53/100  Loss: 0.6472  Acc(last batch): 0.719\n",
      "Epoch 54/100  Loss: 0.6472  Acc(last batch): 0.625\n",
      "Epoch 55/100  Loss: 0.6471  Acc(last batch): 0.531\n",
      "Epoch 56/100  Loss: 0.6471  Acc(last batch): 0.594\n",
      "Epoch 57/100  Loss: 0.6471  Acc(last batch): 0.500\n",
      "Epoch 58/100  Loss: 0.6471  Acc(last batch): 0.656\n",
      "Epoch 59/100  Loss: 0.6470  Acc(last batch): 0.688\n",
      "Epoch 60/100  Loss: 0.6470  Acc(last batch): 0.719\n",
      "Epoch 61/100  Loss: 0.6474  Acc(last batch): 0.781\n",
      "Epoch 62/100  Loss: 0.6470  Acc(last batch): 0.656\n",
      "Epoch 63/100  Loss: 0.6472  Acc(last batch): 0.594\n",
      "Epoch 64/100  Loss: 0.6469  Acc(last batch): 0.656\n",
      "Epoch 65/100  Loss: 0.6472  Acc(last batch): 0.562\n",
      "Epoch 66/100  Loss: 0.6470  Acc(last batch): 0.656\n",
      "Epoch 67/100  Loss: 0.6471  Acc(last batch): 0.688\n",
      "Epoch 68/100  Loss: 0.6470  Acc(last batch): 0.781\n",
      "Epoch 69/100  Loss: 0.6472  Acc(last batch): 0.594\n",
      "Epoch 70/100  Loss: 0.6471  Acc(last batch): 0.625\n",
      "Epoch 71/100  Loss: 0.6470  Acc(last batch): 0.531\n",
      "Epoch 72/100  Loss: 0.6470  Acc(last batch): 0.625\n",
      "Epoch 73/100  Loss: 0.6470  Acc(last batch): 0.594\n",
      "Epoch 74/100  Loss: 0.6471  Acc(last batch): 0.594\n",
      "Epoch 75/100  Loss: 0.6471  Acc(last batch): 0.594\n",
      "Epoch 76/100  Loss: 0.6470  Acc(last batch): 0.656\n",
      "Epoch 77/100  Loss: 0.6471  Acc(last batch): 0.625\n",
      "Epoch 78/100  Loss: 0.6471  Acc(last batch): 0.688\n",
      "Epoch 79/100  Loss: 0.6471  Acc(last batch): 0.656\n",
      "Epoch 80/100  Loss: 0.6472  Acc(last batch): 0.719\n",
      "Epoch 81/100  Loss: 0.6471  Acc(last batch): 0.719\n",
      "Epoch 82/100  Loss: 0.6472  Acc(last batch): 0.531\n",
      "Epoch 83/100  Loss: 0.6473  Acc(last batch): 0.688\n",
      "Epoch 84/100  Loss: 0.6471  Acc(last batch): 0.719\n",
      "Epoch 85/100  Loss: 0.6470  Acc(last batch): 0.625\n",
      "Epoch 86/100  Loss: 0.6471  Acc(last batch): 0.688\n",
      "Epoch 87/100  Loss: 0.6472  Acc(last batch): 0.719\n",
      "Epoch 88/100  Loss: 0.6471  Acc(last batch): 0.656\n",
      "Epoch 89/100  Loss: 0.6469  Acc(last batch): 0.656\n",
      "Epoch 90/100  Loss: 0.6471  Acc(last batch): 0.594\n",
      "Epoch 91/100  Loss: 0.6471  Acc(last batch): 0.625\n",
      "Epoch 92/100  Loss: 0.6472  Acc(last batch): 0.656\n",
      "Epoch 93/100  Loss: 0.6471  Acc(last batch): 0.719\n",
      "Epoch 94/100  Loss: 0.6472  Acc(last batch): 0.625\n",
      "Epoch 95/100  Loss: 0.6471  Acc(last batch): 0.656\n",
      "Epoch 96/100  Loss: 0.6472  Acc(last batch): 0.594\n",
      "Epoch 97/100  Loss: 0.6470  Acc(last batch): 0.625\n",
      "Epoch 98/100  Loss: 0.6472  Acc(last batch): 0.531\n",
      "Epoch 99/100  Loss: 0.6473  Acc(last batch): 0.781\n",
      "Epoch 100/100  Loss: 0.6471  Acc(last batch): 0.656\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # shuffle data every epoch\n",
    "    indices = torch.randperm(n_samples)\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        batch_idx = indices[start:start + batch_size]\n",
    "\n",
    "        Xb = X[batch_idx]          # (B, 8)\n",
    "        yb = y[batch_idx]          # (B, 1)\n",
    "\n",
    "        # 1ï¸âƒ£ clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # -------- forward pass (YOUR CODE) --------\n",
    "        linear1.forward(Xb)\n",
    "        activation1.forward(linear1.output)\n",
    "\n",
    "        linear2.forward(activation1.output)\n",
    "        activation2.forward(linear2.output)\n",
    "\n",
    "        linear3.forward(activation2.output)\n",
    "        activation3.forward(linear3.output)\n",
    "\n",
    "        output_layer.forward(activation3.output)\n",
    "        sigmoid.forward(output_layer.output)\n",
    "\n",
    "        loss = LBC.forward(sigmoid.output, yb)\n",
    "        # -----------------------------------------\n",
    "\n",
    "        # 2ï¸âƒ£ backpropagation (AUTOGRAD)\n",
    "        loss.backward()\n",
    "\n",
    "        # 3ï¸âƒ£ update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "\n",
    "    # quick accuracy check (optional)\n",
    "    preds = (sigmoid.output >= 0.5).float()\n",
    "    acc = (preds == yb).float().mean().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}  Loss: {avg_loss:.4f}  Acc(last batch): {acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
